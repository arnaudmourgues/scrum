<article>
	<preamble>corpus_train/ACL2004-HEADLINE.pdf</preamble>
	<author>Department of Computer ScienceBBN TechnologiesUniversity of Maryland9861 Broken Land Parkway, Suite 156College Park, MD 20742Columbia, MD 21046dmzajic,bonnie @umiacs.umd.eduschwartz@bbn.comstacypre@cs.umd.edu</author>
	<titre>Hybrid Headlines: Combining Topics and Sentence CompressionDavid Zajic, Bonnie Dorr, Stacy PresidentRichard Schwartz</titre>
	<abstract></abstract>
	<biblio>Sabine Bergler, René Witte, Michelle Khalife,Zhuoyan Li, and Frank Rudzicz. 2003. Usingknowledge-poor coreference resolution for text summarization. In Proceedings of the 2003 DocumentUnderstanding Conference, Draft Papers, pages 85–92, Edmonton, Candada.Bonnie Dorr, David Zajic, and Richard Schwartz.2003a. Cross-language headline generation forhindi. ACM Transactions on Asian Language Information Processing (TALIP), 2:2.Bonnie Dorr, David Zajic, and Richard Schwartz.2003b. Hedge trimmer: A parse-and-trim approachto headline generation. In Proceedings of the HLTNAACL 2003 Text Summarization Workshop, Edmonton, Alberta, Canada, pages 1–8.T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13thInternational Workshop on Database and ExpertSystems Applications (DEXA 2002), 2-6 September 2002, Aix-en-Provence, France, pages 215–222.IEEE Computer Society.Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization – step one: Sentence compression. In The 17th National Conference ofthe American Association for Artificial IntelligenceAAAI2000, Austin, Texas.David Lewis. 1992. An evaluation of phrasal and clustered representations on a text categorization task.In Proceedings of the 15th annual internationalACM SIGIR conference on Research and development in information retrieval, pages 37–50, Copenhagen, Denmark.Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation of Summaries Using N-gram CoOccurrences Statistics. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton,Alberta.Ingrid Mårdh. 1980. Headlinese: On the Grammar ofEnglish Front Page Headlines. Malmo.S. Miller, M. Crystal, H. Fox, L. Ramshaw,R. Schwartz, R. Stone, and R. Weischedel. 1998.Algorithms that Learn to Extract Information; BBN:Description of the SIFT System as Used for MUC-7.In Proceedings of the MUC-7.K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of Association ofComputational Linguistics, Philadelphia, PA.R. Schwartz, T. Imai, F. Jubala, L. Nguyen, andJ. Makhoul. 1997. A maximum likelihood modelfor topic classification of broadcast news.InEurospeech-97, Rhodes, Greece.David Zajic, Bonnie Dorr, Richard Schwartz, and StacyPresident. 2004. Headline evaluation experimentresults, umiacs-tr-2004-18. Technical report, University of Maryland Institute for Advanced Computing Studies, College Park, Maryland.Liang Zhou and Eduard Hovy. 2003. Headline summarization at isi. In Proceedings of the 2003 Document Understanding Conference, Draft Papers,pages 174–178, Edmonton, Candada.</biblio>
</article>
<article>
	<preamble>corpus_train/Boudin-Torres-2006.pdf</preamble>
	<author>\Laboratoire Informatique d’Avignon339 chemin des Meinajaries, BP1228,84911 Avignon Cedex 9, France.</author>
	<titre>A Scalable MMR Approach to Sentence Scoringfor Multi-Document Update SummarizationFlorian Boudin \ and Marc El-Bèze \</titre>
	<abstract></abstract>
	<biblio>Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUSat DUC 2005: Understanding documents via concept links. In Document Understanding Conference(DUC).Boudin, F. and J.M. Torres-Moreno. 2007. A Cosine Maximization-Minimization approach for UserOriented Multi-Document Update Summarization.In Recent Advances in Natural Language Processing(RANLP), pages 81–87.Carbonell, J. and J. Goldstein. 1998. The use of MMR,diversity-based reranking for reordering documentsand producing summaries. In 21st annual international ACM SIGIR conference on Research and de-</biblio>
</article>
<article>
	<preamble>corpus_train/compression.pdf</preamble>
	<author>1</author>
	<titre>Multi-Candidate Reduction: Sentence Compression as a Tool forDocument Summarization Tasks∗David Zajic1 , Bonnie J. Dorr1 , Jimmy Lin1 , Richard Schwartz2</titre>
	<abstract></abstract>
	<biblio>the same source sentences are available as candidates for inclusion in the final summary.Minimization of redundancy is an important element of a multi-document summarization system.Carbonell and Goldstein (1998) propose a technique called Maximal Marginal Relevance (MMR) forranking documents returned by an information retrieval system so that the front of the ranked list willcontain diversity as well as high relevance. Goldstein et al. (2000) extend MMR to multi-documentsummarization. MCR borrows the ranking approach of MMR, but uses a different set of features. LikeMEAD, these approaches use feature weights that are optimized to maximize an automatic metric ontraining data.Several researchers have shown the importance of summarization in domains other than writtennews (Muresan et al., 2001; Clarke and Lapata, 2006). Within the MCR framework, we discuss theportability of Trimmer and HMM Hedge to a variety of different texts: written news, broadcast newstranscriptions, email threads, and text in foreign language.43Single-Sentence CompressionOur general approach to the generation of a summary from a single document is to produce a headlineby selecting words in order from the text of the story. Consider the following excerpt from a news storyand corresponding headline:(1)(i)(ii)After months of debate following the Sept. 11 terrorist hijackings, the TransportationDepartment has decided that airline pilots will not be allowed to have guns in thecockpits.Pilots not allowed to have guns in cockpits.The bold words in (1i) form a fluent and accurate headline, as shown in (1ii).This basic approach has been realized in two ways. The first, Trimmer, uses a linguisticallymotivated algorithm to remove grammatical constituents from the lead sentence until a length thresholdis met. Topiary is a variant of Trimmer that combines fluent text from a compressed sentence withtopic terms to produce headlines. The second, HMM Hedge, employs a noisy-channel model to findthe most likely headline for a given story. The remainder of this section will present Trimmer, Topiary,and HMM Hedge in more detail.3.1TrimmerOur first approach to sentence compression involves iteratively removing grammatical constituents fromthe parse tree of a sentence using linguistically-motivated rules until a length threshold has been met.When applied to the lead sentence, or first non-trivial sentence of a story, our algorithm generates avery short summary, or headline. This idea is implemented in our Trimmer system, which can leveragethe output of any constituency parser that uses the Penn Treebank conventions. At present we useCharniak’s parser (Charniak, 2000).The insights that form the basis and justification for the Trimmer rules come from our previousstudy, which compared the relative prevalence of certain constructions in human-written summariesand lead sentences in stories. This study used 218 human-written summaries of 73 documents fromthe TIPSTER corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries andthe lead sentences of the 73 stories were parsed using the BBN SIFT parser (Miller et al., 2000). Theparser produced 957 noun phrases (NP nodes in the parse trees) and 315 clauses (S nodes in the parsetrees) for the 218 summaries. For the 73 lead sentences, the parser produced 817 noun phrases and316 clauses.At each level (sentence, clause, and noun phrase), different types of linguistic phenomena werecounted.• At the sentence level, the numbers of preposed adjuncts, conjoined clauses, and conjoined verbphrases were counted. Children of the root S node that occur to the left of the first NP areconsidered to be preposed adjuncts. The bracketed phase in “[According to police] the crime ratehas gone down” is a prototypical example of a preposed adjunct.• At the clause level, temporal expressions, trailing SBAR nodes, and trailing PP nodes werecounted. Trailing constituents are those not designated as an argument of a verb phrase.• At both the sentence and clause levels, conjoined S nodes and conjoined VP nodes were counted.• At the NP level, determiners and relative clauses were counted.The counts and prevalence of the phenomena in the human-generated headlines and lead sentencesare shown in Table 1. The results of this analysis illuminated the opportunities for trimming constituents and guided the development of our Trimmer rules, detailed below.5LevelSentenceClauseNoun PhrasePhenomenonpreposed adjunctsconjoined Sconjoined VPtemporal expressiontrailing PPtrailing SBARrelative clausedeterminerSummary0/2180%1/2180.5%7/2183%5/3151.5%165/315 52%24/3158%3/9570.3%31/9573%Lead Sentence2/732.7%3/734%20/7327%77/31624%184/316 58%49/31616%29/8173.5%205/817 25%Table 1: Counts and prevalence of phenomena found in summaries and lead sentences.3.1.1Trimmer AlgorithmTrimmer applies syntactic compression rules to a parse tree according the following algorithm:1. Remove temporal expressions2. Select Root S node3. Remove preposed adjuncts4. Remove some determiners5. Remove conjunctions6. Remove modal verbs7. Remove complementizer that8. Apply the XP over XP rule9. Remove PPs that do not contain named entities10. Remove all PPs under SBARs11. Remove SBARSs12. Backtrack to state before Step 913. Remove SBARs14. Remove PPs that do not contain named entities15. Remove all PPsSteps 1 and 4 of the algorithm remove low-content units from the parse tree.Temporal expressions—although certainly not content-free—are not usually vital for summarizingthe content of an article. Since the goal is to provide an informative headline, the identification andelimination of temporal expressions (Step 1) allow other more important details to remain in the lengthconstrained headline. The use of BBN’s IdentiFinderTM (Bikel et al., 1999) for removal of temporalexpressions is described in Section 3.1.2.The determiner rule (Step 4) removes leaf nodes that are assigned the part-of-speech tag DT andhave the surface form the, a, or an. The intuition for this rule is that the information carried by6articles is expendable in summaries, even though this makes the summaries ungrammatical for generalEnglish. Omitting articles is one of the most salient features of newspaper headlines. Sentences (2)and (3), taken from the New York Times website on September 27, 2006, illustrate this phenomenon.The italicized articles did not occur in the actual newspaper headlines.(2)The Gotti Case Ends With a Mistrial for the Third Time in a Year(3)A Texas Case Involving Marital Counseling Is the Latest to Test the Line Between Church andStateStep 2 identifies nodes in the parse tree of a sentence that could serve as the root of a compressionfor the sentence. Such nodes will be referred to as Root S nodes. A node in a tree is a Root S node if itis labeled S in the parse tree and has children labeled NP and VP, in that order. The human-generatedheadlines we studied always conform to this rule. It has been adopted as a constraint in the Trimmeralgorithm that the lowest leftmost Root S node is taken to be the root node of the headline. Anexample of this rule application is shown in (4). The boldfaced material in the parse is retained andthe italicized material is eliminated.(4)(i)Input: Rebels agreed to talks with government officials, international observers said Tuesday.(ii)Parse: [S [S [NP Rebels][VP agreed to talks with government officials]], international observers said Tuesday.](iii) Output: Rebels agreed to talks with government officials.When the parser produces a usable parse tree, this rule selects a valid starting point for compression.However, the parser sometimes produces incorrect output, as in the cases below (from DUC-2003):(5)(i)Parse: [S[SBAR What started as a local controversy][VP has evolved into aninternational scandal.]](ii)Parse: [NP[NP Bangladesh][CC and][NP[NP India][VP signed a water sharingaccord.]]]In (5i), an S exists, but it does not conform to the requirements of the Root S rule because it doesnot have as children an NP followed by a VP. The problem is resolved by selecting the lowest leftmostS, ignoring the constraints on the children. In (5ii), no S is present in the parse. This problem isresolved by selecting the root of the entire parse tree as the root of the headline. These parsing errorsoccur infrequently—only 6% of the sentences in the DUC-2003 evaluation data exhibit these problems,based on parses generated by the BBN SIFT parser.The motivation for removing preposed adjuncts (Step 3) is that all of the human-generated headlinesomit what we refer to as the preamble of the sentence. Preposed adjuncts are constituents that precedethe first NP (the subject) under the Root S chosen in Step 2; the preamble of a sentence consists of itspreposed adjuncts. The impact of preposed adjunct removal can be seen in example (6).(6)(i)Input: According to a now finalized blueprint described by U.S. officials and other sources,the Bush administration plans to take complete, unilateral control of a post-Saddam HusseinIraq.(ii)Parse: [S[PP According to a now finalized blueprint described by U.S. officials and othersources], [Det the] Bush administration plans to take complete, unilateral controlof[Det a] post-Saddam Hussein Iraq.]7(iii) Output: Bush administration plans to take complete unilateral control of post-SaddamHussein Iraq.The remaining steps of the algorithm remove linguistically peripheral material through successivedeletions of constituents until the sentence is shorter than a length threshold. Each stage of thealgorithm corresponds to the application of one of the rules. Trimmer first finds the pool of nodes inthe parse to which a rule can be applied. The rule is then iteratively applied to the deepest, rightmostremaining node in the pool until the length threshold is reached or the pool is exhausted. After a rulehas been applied at all possible nodes in the parse tree, the algorithm moves to the next step.In the case of a conjunction with two children (Step 5), one of the children will be removed. If theconjunction is and , the second child is removed. If the conjunction is but, the first child is removed.This rule is illustrated by the following examples, where the italicized text is trimmed.(7)When Sotheby’s sold a Harry S Truman signature that turned out to be a reproduction, theprestigious auction house apologized and bought it back .(8)President Clinton expressed sympathy after a car-bomb explosion in a Jerusalem market wounded24 people but said the attack should not derail the recent land-for-security deal between Israeland the Palestinians.The modal verb rule (Step 6) applies to verb phrases in which the head is a modal verb and thehead of the child verb phrase is a form of have or be. In such cases, the modal and auxiliary verbsare removed. Sentences (9) and (10) show examples of this rule application. Note that although inSentence (10) the omission of trimmed material changes the meaning, given a tight space constraint,the loss of the modality is preferable to the loss of other content information.(9)People’s palms and fingerprints may be used to diagnose schizophrenia.(10) Agents may have fired potentially flammable tear gas cannisters.The complementizer rule (Step 7) removes the word that when it occurs as a complementizer.Sentence (11) shows an example in which two complementizers can be removed.(11) Hoffman stressed that study is only preliminary and can’t prove that treatment useful.The XP-over-XP rule (Step 8) is a linguistic generalization that allows a single rule to cover twodifferent phenomena. XP in the name of the rule is a variable that can take two values: NP and VP.In constructions of the form [XP [XP ...] ...], the other children of the higher XP are removed. Notethat the child XP must be the first child of the parent XP. When XP = NP the rule removes relativeclauses (as in Sentence (12)) and appositives (as in Sentence (13)).(12) Schizophrenia patients whose medication couldn’t stop the imaginary voices in their heads gainedsome relief.(13) A team led by Dr. Linda Brzustowicz, assistant professor of neuroscience at Rutgers University’sCenter for Molecular and Behavioral Neuroscience in Newark, studied DNA of dozens of membersof 22 families.The rules that remove prepositional phrases and subordinate clauses (Steps 9 through 15) aresometimes prone to removing important content. Thus, these rules are applied last, only when thereare no other types of rules to apply. Moreover, these rules are applied with a backoff option to avoid8over-trimming the parse tree. First, the PP rule is applied (Steps 9 and 10),1 followed by the SBARrule (Step 11). If the desired sentence length has not been reached, the system reverts to the parse treeas it was before any prepositional phrases were removed (Step 12) and applies the SBAR rule (Step13). If the desired length still has not been reached, the PP rule is applied again (Steps 14 and 15).The intuition behind this ordering is that, when removing constituents from a parse tree, it is preferable to remove smaller fragments before larger ones and prepositional phrases tend to be smaller thansubordinate clauses. Thus, Trimmer first attempts to achieve the desired length by removing smallerconstituents (PPs), but if this cannot be accomplished, the system restores the smaller constituents,removes a larger constituent, and then resumes the deletion of the smaller constituents. To reduce therisk of removing prepositional phrases that contain important information, BBN’s IdentiFinder is usedto distinguish PPs containing temporal expressions and named entities, as described next.3.1.2Use of BBN’s IdentiFinder in TrimmerBBN’s IdentiFinder is used both for the elimination of temporal expressions and for conservativedeletion of PPs containing named entities. The elimination of temporal expressions (Step 1) is a twostep process: (a) use IdentiFinder to mark temporal expressions; and (b) remove [PP ... [NP [X] ...]...] and [NP [X]] where X is tagged as part of a temporal expression. The following examples illustratethe application of temporal expression removal rule:(14) (i)(ii)Input: The State Department on Friday lifted the ban it had imposed on foreign fliers.Parse: [S [NP[Det The] State Department [PP [IN on] [NP [NNP Friday]]] [VP lifted[Det the] ban it had imposed on foreign fliers.]](iii) Output: State Department lifted ban it had imposed on foreign fliers.(15) (i)Input: An international relief agency announced Wednesday that it is withdrawing fromNorth Korea.(ii) Parse: [S [NP [Det An] international relief agency][VP announced [NP [NNP Wednesday]] that it is withdrawing from North Korea.]](iii) Output: International relief agency announced that it is withdrawing from North Korea.IdentiFinder is also used to ensure that prepositional phrases containing named entities are notremoved during the first round of PP removal (Step 9). However, prepositional phrases containingnamed entities that are descendants of SBARs are removed before the parent SBAR is removed, sincewe should remove a smaller constituent before removing a larger constituent that subsumes it. Sentence(16) shows an example of a SBAR subsuming two PPs, one of which contains a named entity.(16) The commercial fishing restrictions in Washington will not be lifted [SBAR unless the salmonpopulation increases [PP to a sustainable number] [PP in the Columbia River]].If the PP rule were not sensitive to named entities, the PP in the Columbia River would be thefirst prepositional phrase to be removed, because it is the lowest rightmost PP in the parse. However,this PP provides an important piece of information: the location of the salmon population. The rule inStep 9 will skip the last prepositional phrase and remove the penultimate PP to a sustainable number .This concludes an overview of the Trimmer rules and our syntactic sentence compression algorithm.Given a length limit, the system will produce a single compressed version of the target sentence.Multiple compressions can be generated by setting the length limit to be very small and storing the stateof the sentence after each rule application as a compressed variant. In section 4, we will describe howmultiple compressed candidates generated by Trimmer are used as a component of a multi-documentsummarization system.1The reason for breaking PP removal into two stages is discussed in Section 3.1.2.93.2TopiaryWe have used the Trimmer approach to compression in another variant of single-sentence summarizationcalled Topiary. This system combines Trimmer with a topic discovery approach (described next) toproduce a fluent summary along with additional context.The Trimmer algorithm is constrained to build a headline from a single sentence. However, it isoften the case that no single sentence contains all the important information in a story. Relevantinformation can be spread over multiple sentences, linked by anaphora or ellipsis. In addition, thechoice of lead sentence may not be ideal and our trimming rules are imperfect.On the other hand, approaches that construct headlines from lists of topic terms (Lewis, 1999;Schwartz et al., 1997) also have limitations. For example, Unsupervised Topic Discovery (UTD)—described below—rarely generates any topic terms that are verbs. Thus, topic lists are good at indicating the general subject but rarely give any direct indication of what events took place. Intuitively,we need both fluent text to tell what happened and topic terms to provide context.3.2.1Topic Term Generation: UTD and OnTopicOnTopic (Schwartz et al., 1997) uses an HMM to assign topics to a document; topic models are derivedfrom an annotated corpus. However, it is often difficult to acquire such data, especially for a new genreor language. UTD (Sista et al., 2002) was developed to overcome this limitation: it takes as input alarge unannotated corpus and automatically creates a set of topic models with meaningful names.The UTD algorithm has several stages. First, it analyzes the corpus to find multi-word sequencesthat can be treated as single tokens. It does this using two methods. One method is a minimumdescription length criterion that detects phrases that occur frequently relative to the individual words.The second method uses BBN’s IdentiFinder to detect multi-word names. These names are added tothe text as additional tokens. They are also likely to be chosen as potential topic names. In the secondstage of UTD, we find those terms (both single-word and multi-word) with high tf.idf. Only thosetopic names that occur as high-content terms in at least four different documents are kept. The thirdstage trains topic models corresponding to these topic terms. The modified Expectation Maximizationprocedure of BBN’s OnTopic system is used to determine which words in the documents often signifythese topic names. This produces topic models. Fourth, these topic models are used to find the mostlikely topics for each document, which is equivalent to assigning the name of the topic model to thedocument as a topic term. This often assigns topics to documents where the topic name does not occurin the document text.We found, in various experiments (Sista et al., 2002), that the topic names derived by this procedurewere usually meaningful and that the topic assignment was about as good as when the topics werederived from a corpus that was annotated by people. We have also used this procedure on differentlanguages and shown the same behavior. Since UTD is unsupervised, it can run equally well on a newlanguage, as long as the documents can be divided into strings that approximate words.The topic list in (17) was generated by UTD and OnTopic for a story about the FBI investigationof the 1998 bombing of the U.S. embassy in Nairobi.(17) BIN LADEN, EMBASSY, BOMBING, POLICE OFFICIALS, PRISON, HOUSE, FIRE, KABILATopiary uses UTD to generate topic terms for the collection of documents to be summarized anduses OnTopic to assign the topic terms to the documents. The next section will describe how topicterms and sentence compressions are combined to form Topiary summaries.103.2.2Topiary AlgorithmAs each Trimmer rule is applied to a sentence, the resulting state of the sentence is stored as acompressed variant of the source sentence. Topiary selects from the variants the longest one suchthat there is room to prepend the highest scoring non-redundant topic term. Suppose the highestscoring topic term is “terrorism” and the length threshold is 75 characters. To make room for the topic“terrorism”, the length threshold is lowered by 10 characters: 9 characters for the topic and 1 characteras a separator. Thus, Topiary chooses the longest trimmed variant under 65 characters that does notcontain the word “terrorism”, If there is no such candidate, i.e., all the trimmed variants contain theword terrorism, Topiary would consider the second highest scoring topic word, “bomb”. Topiary wouldselect the longest trimmed variant under 70 characters that does not contain the word “bomb”. AfterTopiary has selected a trimmed variant and prepended a topic to it, it checks to see how much unusedspace remains under the threshold. Additional topic words are added between the first topic word andthe compressed sentence until all space is exhausted.This process results in a headline that contains one or more main topics about the story and ashort sentence that says what happened concerning them. The combination is often more concise thana fully fluent sentence and compensates for the fact that the information content from the topic andthe compressed sentence do not occur together in any single sentence from the source text.As examples, sentences (18) and (19) are the outputs of Trimmer and Topiary, respectively, for thesame story in which UTD selected the topic terms in (17).(18) FBI agents this week began questioning relatives of the victims(19) BIN LADEN, EMBASSY, BOMBING: FBI agents this week began questioning relativesBy combining topics and parse-and-trim compression, Topiary achieved the highest score on thesingle-document summarization task (i.e., headline generation task) in DUC-2004 (Zajic et al., 2004).3.3HMM HedgeOur second approach to sentence compression, implemented in HMM Hedge, treats the observed data(the story) as the result of unobserved data (headlines) that have been distorted by transmissionthrough a noisy channel. The effect of the noisy channel is to add story words between the headlinewords. The model is biased by parameters to make the resulting headlines more like Headlinese, theobserved language of newspaper headlines created by copy editors.Formally, we consider a story S to be a sequence of N words. We want to find a headline H, asubsequence of words from S, that maximizes the likelihood that H generated the story S, or:argmaxH P (H|S)It is difficult to directly estimate P (H|S), but this probability can be expressed in terms of otherprobabilities that are easier to compute, using Bayes’ rule:P (H|S) =P (S|H)P (H)P (S)Since the goal is to maximize this expression over H, and P (S) is constant with respect to H, thedenominator of the above expression can be omitted. Thus we wish to find:argmaxH P (S|H)P (H)Let H be a headline consisting of words h1 , h2 , ..., hn . Let the special symbols start and end representthe beginning and end of a headline, respectively. P (H) can be estimated using a bigram model ofHeadlinese:P (H) = P (h1 |start)P (h2 |h1 )...P (end|hn )11The bigram probabilities of the words in the headline language were computed from a corpus ofEnglish headlines taken from 242,918 AP newswire stories in the TIPSTER corpus. The headlinescontain 2,848,194 words from a vocabulary of 88,627 distinct words.Given a story S and a headline H, the action of the noisy channel is to form S by adding non-headlinewords to H. Let G be the non-headline words added by the channel to the headline: g1 , g2 , ..., gm . Forthe moment, we assume that the headline words are transmitted through the channel with probability1. We estimate P (S|H), the probability that the channel added non-headline words G to headline Hto form story S. This is accomplished using a unigram model of newspaper stories that we will referto as the general language, in contrast to the headline language. Let Pgl (g) be the probability ofnon-headline word g in the general language and Pch (h) = 1 be the probability that headline word his transmitted through the channel as story word h.P (S|H) = Pgl (g1 )Pgl (g2 )...Pgl (gm )Pch (h1 )Pch (h2 )...Pch (hn )= Pgl (g1 )Pgl (g2 )...Pgl (gm )The unigram probabilities of the words in the general language were computed from 242,918 EnglishAP news stories in the TIPSTER corpus. The stories contain 135,150,288 words from a vocabulary of428,633 distinct words.The process by which the noisy channel generates a story from a headline can be represented by aHidden Markov Model (HMM) (Baum, 1972). An HMM is a weighted finite-state automaton in whicheach state probabilistically emits a string. The simplest HMM for generating headlines consists of twostates: an H state that emits words that occur in the headline and a G state that emits all the otherwords in the story.Since we use a bigram model of headlines, each state that emits headline words must “remember”the previously emitted headline word. If we did not constrain headline words to actually occur in thestory, we would need an H state for each word in the headline vocabulary. However, because headlinewords are chosen from the story words, it is sufficient to have an H state for each story word. For anystory, the HMM consists of a start state S, an end state E, an H state for each word in the story, acorresponding G state for each H state, and a state Gstart that emits words that occur before the firstheadline word in the story. An H state can emit only the word it represents. The corresponding Gstate remembers which word was emitted by its H state and can emit any word in the story language.A headline corresponds to a path through the HMM from S to E that emits all the words in the storyin the correct order. In practice the HMM is constructed with states for only the first N words of thestory, where N is a constant (60), or N is the number of words in the first sentence.2In example (1i), given earlier, the H states will emit the words in bold (pilots, not, allowed, to,have, guns, in, cockpits) and the G states will emit all the other words. The HMM will transitionbetween the H and G states as needed to generate the words of the story. In the current example,the model will have states Start, Gstart , End, and 28 H states with 28 corresponding G states.3 Theheadline given in example (1ii) corresponds to the following sequence of states: Start, Gstart 17 times,Hpilots , Gpilots , Hnot , Gnot , Hallowed , Hto , Hhave , Hguns , Hin , Gin , Hcockpits , End. This path is not theonly one that could generate the story in (1i). Other possibilities are:(20) (i)(ii)Transportation Department decided airline pilots not to have guns.Months of the terrorist has to have cockpits.2Limiting consideration of headline words to the early part of the story is justified in Dorr et al. (2003a) where itwas shown that more than half of the headline words are chosen from the first sentence of the story. Other methods forselecting the window of story words are possible and will be explored in future research.3The subscript of a G state indicates the H state it is associated with, not the story word it emits. In the example,Gpilots emits story word will , Gnot emits story word be, and Gin emits story word the.12Although (20i) and (20ii) are possible headlines for (1i), the conditional probability of (20ii) given (1i)will be lower than the conditional probability of (20i) given (1i).The Viterbi algorithm (Viterbi, 1967) is used to select the most likely headline for a given story.We use length constraints to find the most likely headlines consisting of W words, where W rangesfrom 5 to 15. Multiple backpointers are used so that we can find the n most likely headlines at eachlength.HMM Hedge is enhanced by three additional decoding parameters to help the system choose outputsthat best mimic actual headlines: a position bias, a clump bias, and a gap bias. The incorporationof these biases changes the score produced by the decoder from a probability to a relative desirabilityscore. The three parameters were motivated by analysis of system output and their values were set bytrial and error. A logical extension to this work would be to learn the best setting of these biases, e.g.,through Expectation Maximization.The position bias favors headlines that include words near the front of the story. This reflects ourobservations of human-constructed headlines, in which headline words tend to appear near the frontof the story. The initial position bias p is a positive number less than one. The story word in thenth position is assigned a position bias of log(pn ). When an H state emits a story word, the positionbias is added to the desirability score. Thus, words near the front of the story carry less of a positionbias than words farther along. Note that this generalization often does not hold in the case of humaninterest and sports stories, which may start with a hook to get the reader’s attention, rather than atopic sentence.We also observed that human-constructed headlines tend to contain contiguous blocks of storywords. Example (1ii), given earlier, illustrates this with the string “allowed to have guns”. Thestring bias is used to favor “clumpiness”. i.e., the tendency to generate headlines composed of clumpsof contiguous story words. The log of the clump bias is added to the desirability score with eachtransition from an H state to its associated G state. With high clump biases, the system will favorheadlines consisting of fewer but larger clumps of contiguous story words.The gap bias is used to disfavor headline “gappiness”, i.e., large gaps of non-headline words in thestory between clumps of headline words. Although humans are capable of constructing fluent headlinesby selecting widely spaced words, we observed that HMM Hedge was more likely to combine unrelatedmaterial by doing this. At each transition from a G state to an H state, corresponding to the endof a sequence of non-headline words in the story, a gap bias is applied that increases with the size ofthe gap between the current headline and the last headline word to be emitted. This can also be seenas a penalty for spending too much time in one G state. With high gap biases, the system will favorheadlines with few large gaps.One characteristic difference between newspaper headline text and newspaper story text is thatheadlines tend to be in present tense while story sentences tend to be in the past tense. Past tenseverbs occur more rarely in the headline language than in the general language. HMM Hedge mimics thisaspect of Headlinese by allowing morphological variation between headline verbs and the correspondingstory verbs. Morphological variation for verbs is achieved by creating an H state for each availablevariant of a story verb. These H states still emit the story verb but they are labeled with the variant.HMM Hedge can generate a headline in which proposes is the unobserved headline word that emits theobserved story word proposed , even though proposes does not occur in the story.(21) (i)(ii)A group has proposed awarding $1 million in every general election to one randomly chosenvoter.Group proposes awarding $1 million to randomly chosen voter.Finally, we constrain each headline to contain at least one verb. That is to say, we ignore headlinesthat do not contain at least one verb, no matter how desirable the decoding is.13Although we have described an application of HMM Hedge to blocks of story words without referenceto sentence boundaries, it is also possible to use HMM as a single sentence compressor by limiting theblock to the words in a single sentence.Also, as we will see shortly, multiple alternative compressions of a sentence may be generated withHMM Hedge. The Viterbi algorithm is capable of discovering n-best compressions of a window ofstory words and can be constrained to consider only paths that include a specific number of H states,corresponding to compressions that contain a specific number of words. We use HMM Hedge to generate55 compressions for each sentence by computing the five best headlines at each length, from 5 to 15words. In Section 4 we will describe how HMM Hedge is used as a component of a multi-documentsummarization system.4Multi-Document SummarizationThe sentence compression tools we developed for single-document summarization have been incorporated into our Multi-Candidate Reduction framework for multi-document summarization. MCRproduces a textual summary from a collection of relevant documents in three steps. First, sentencesare selected from the source documents for compression. The most important information occurs nearthe front of the stories, so we select the first five sentences of each document for compression. Second,multiple compressed versions of each sentence are produced using Trimmer or HMM Hedge to create apool of candidates for inclusion in the summary. Finally, a sentence selector constructs the summaryby iteratively choosing from the pool of candidates based on a linear combination of features until thesummary reaches a desired length.At present, weights for the features are determined by manually optimizing on a set of trainingdata to maximize the Rouge-2 recall score (Lin and Hovy, 2003), using Rouge version 1.5.5. Atypical summarization task might call for the system to generate a 250-word summary from a coupleof dozen news stories. These summaries may be query-focused, in the sense that the summaries shouldbe responsive to a particular information need, or they may be generic, in that a broad overview of thedocuments is desired.Our sentence selector adopts certain aspects of Maximal Marginal Relevance (MMR) (Carbonelland Goldstein, 1998), an approach that attempts to balance relevance and anti-redundancy. In MCR’sselection module, the highest scoring sentence from the pool of eligible candidates is chosen for inclusionin the summary. Features that contribute to a candidate’s score can be divided into two types: dynamicand static. When a candidate is chosen, all other compressed variants of that sentence are eliminated.After a candidate is added to the summary, the dynamic features are re-computed, and the candidatesare re-ranked. Candidates are added to the summary until the desired length is achieved. The orderingof candidates in the summary is the same as the order in which they were selected for inclusion. Thefinal sentence of the summary is truncated if it causes the summary to exceed the length limit.4.1Static FeaturesStatic features are calculated before sentence selection begins and do not change during the process ofsummary construction:• Position. The zero-based position of the sentence in the document.• Sentence Relevance. The relevance score of the sentence to the query.• Document Relevance. The relevance score of the document to the query.• Sentence Centrality. The centrality score of the sentence to the topic cluster.14• Document Centrality. The centrality score of the document to the topic cluster.• Scores from the Compression Modules:– Trim rule application counts. For Trimmer-based MCR, the number of Trimmer rule instances applied to produce the candidate.– Negative Log Desirability. For HMM-based MCR, the relative desirability score of thecandidate.We use the Uniform Retrieval Architecture (URA), University of Maryland’s software infrastructurefor information retrieval tasks, to compute relevance and centrality scores for each compressed candidate. There are four such scores: the relevance score between a compressed sentence and the query,the relevance score between the document containing the compressed sentence and the query, the centrality score between a compressed sentence and the topic cluster, and the centrality score betweenthe document containing the compressed sentence and the topic cluster. We define the topic clusterto be the entire collection of documents relevant to this particular summarization task. Centrality isa concept that quantifies how similar a piece of text is to all other texts that discuss the same generaltopic. We assume that sentences having higher term overlap with the query and sources more “central”to the topic cluster are preferred for inclusion in the final summary.The relevance score between a compressed sentence and the query is an idf-weighted count ofoverlapping terms (number of terms shared by the two text segments). Inverse document frequency(idf), a commonly-used measure in the information retrieval literature, roughly captures term salience.The idf of a term t is defined by log(N/ct ), where N is the total number of documents in a particularcorpus and ct is the number of documents containing term t; these statistics were calculated from oneyear’s worth of LA Times articles. Weighting term overlap by inverse document frequency capturesthe intuition that matching certain terms is more important than matching others.Lucene, a freely-available off-the-shelf information retrieval system, is used to compute the threeother scores. The relevance score between the document containing the compressed sentence andthe query is computed using Lucene’s built-in similarity function. The centrality score between thecompressed sentence and the topic cluster is the mean of the similarity between the sentence and eachdocument comprising the cluster (once again, as computed by Lucene’s built-in similarity function).The document-cluster centrality score is also computed in much the same way, by taking the meanof the similarity of the particular document with every other document in the cluster. In order toobtain an accurate distribution of term frequencies to facilitate the similarity calculation, we indexedall relevant documents (i.e., the topic cluster) along with a comparable corpus (one year of the LATimes)—this additional text essentially serves as a background model for non-relevant documents.Some features are derived from the sentence compression modules used to generate candidates. ForTrimmer, the rule application count feature of a candidate is the number of rules that were applied toa source sentence to produce the candidate. The rules are not presumed to be equally effective, so therule application counts are broken down by rule type. For HMM Hedge, we use the relative desirabilityscore calculated by the decoder, expressed as a negative log.The features discussed in this section are assigned to the candidates before summary generationbegins and remain fixed throughout the process of summary sentence selection. The next sectiondiscusses how candidate features are assigned new values as summary geneneration proceeds.4.2Dynamic FeaturesDynamic features change during the process of sentence selection to reflect changes in the state of thesummary as sentences are added.4 The dynamic features are:4At present the dynamic features are properties of the candidates, calculated with respect to the current summarystate. There are no features directly relating to the amount of space left in the summary, so there is no mechanism that15• Redundancy. A measure of how similar the sentence is to the current summary.• Sentence-from-doc. The number of sentences already selected from the sentence’s document.The intuition behind our redundancy measure is that candidates containing words that occur muchmore frequently in the current state of the summary than they do in general English are redundantto the summary. We imagine that sentences in the summary are generated by the underlying worddistribution of the summary rather than the distribution of words in the general language. If a sentenceappears to have been generated by the summary rather than by the general language, we take it tobe redundant to the summary. Suppose we have a summary about earthquakes. The presence in acandidate of words like earthquake, seismic, and Richter Scale, which have a high likelihood in thesummary, will make us think that the candidate is redundant to the summary.To estimate the extent to which a candidate is more likely to have been generated by a summarythan by the general language, we consider the probabilities of the words in the candidate. We estimatethat the probability that a word w occurs in a candidate generated by the summary isP (w) = λP (w|D) + (1 − λ)P (w|C)where D is the summary, C is the general language corpus5 , λ is a parameter estimating the probabilitythat the word was generated by the summary and (1−λ) is the probability that the word was generatedby the general language. We have set λ = 0.3, as a general estimate of the portion of words in a textthat are specific to the text’s topic. We estimate the probabilities by counting the words6 in the currentsummary and the general language corpus:P (w|D) =count of w in Dsize of DP (w|C) =count of w in Csize of CWe take the probability of a sentence to be the product of the probabilities of its words, so we calculatethe probability that a sentence was generated by the summary, i.e. our redundancy metric, as:YRedundancy(S) =λP (s|D) + (1 − λ)P (s|C)s∈SFor ease of computation, we actually use log probabilities:Xlog(λP (s|D) + (1 − λ)P (s|C))s∈SRedundancy is a dynamic feature because the word distribution of the current summary changes withevery iteration of the sentence selector.4.3Examples of System OutputWe applied our MCR framework to test data from the DUC-2006 evaluation (Dang and Harman, 2006).Given a topic description and a set of 25 documents related to the topic (drawn from AP newswire,the New York Times, and the Xinhua News Agency English Service), the system’s task was to createwould affect the distribution of compressed candidates over the iterations of the sentence selector. This issue will beaddressed as future work in Section 7.5The documents in the set being summarized are used to estimate the general language model.6Actually, preprocessing for redundancy includes stopword removal and applying the Porter Stemmer (Porter, 1980).16Title: Native American Reservation System—pros and consNarrative Description: Discuss conditions on American Indian reservations or among NativeAmerican communities. Include the benefits and drawbacks of the reservation system. Includelegal privileges and problems.Figure 2: Topic D0601A from the DUC-2006 multi-document summarization task.a 250-word summary that addressed the information need expressed in the topic. One of the topicdescriptions is shown in Figure 2. The 25 documents in the document set have an average size of 1170words, so a 250-word summary represents a compression ratio of 0.86%.Figures 3, 4 and 5 show examples of MCR output using Trimmer compression, HMM Hedge compression, or no compression. For readability, we use ◦ as a sentence delimiter; this is not part of theactual system output. The sentences compressed by Trimmer mimic Headlinese by omitting determiners and auxiliary verbs. For example, the first sentence in Figure 3 is a compression of the followingsource sentence:Seeking to get a more accurate count of the country’s American Indian population, theCensus Bureau is turning to tribal leaders and residents on reservations to help overcomelong-standing feelings of wariness or anger toward the federal government.Three determiners and a form of be have been removed from the source sentence in the compressionthat appears in the summary. The removal of this material makes the sentence appear more like aheadline.In comparison with Trimmer compressions, HMM compressions are generally less readable andmore likely to be misleading. Consider the final sentence in Figure 4.(22) main purpose of reservation to pay American Indians by poverty proposalsThis is a compression of the following source sentence:(23) But the main purpose of the visit—the first to a reservation by a president since FranklinRoosevelt—was simply to pay attention to American Indians, who are so raked by grindingpoverty that Clinton’s own advisers suggested he come up with special proposals geared specifically to the Indians’ plight.Because HMM Hedge uses a bigram model of Headlines, it is unable to capture sentence-levelgrammaticality. The same limitation makes it difficult to prevent misleading or incorrect compressions.For example, the third sentence from the end of Figure 4 seems to say that a court legalized gamblingon Indian reservations:(24) Supreme Court allows legalized gambling Indian reservationsHowever, it is a compression of the following source sentence:(25) Only Monday, the California Supreme Court overturned a ballot measure that would have allowedexpansion of legalized gambling on Indian reservations.Nevertheless, we can see from the examples that sentence compression allows a summary to includemore material from other sources. This increases the topic coverage of system output.17Seeking to get more accurate count of country’s American Indian population, Census Bureau turning totribal leaders and residents on reservations to help overcome long-standing feelings. ◦ American Indianreservations would get infusion. ◦ Smith and thousands seeking help for substance abuse at AmericanIndian Community House, largest of handful of Native American cultural institutions. ◦ Clinton goingto Pine Ridge Reservation for visit with Oglala Sioux nation and to participate in conference on NativeAmerican homeownership and economic development. ◦ Said Glen Revere, nutritionist with Indian HealthServices on 2.8 million-acre Wind River Reservation, about 100 miles east of Jackson, Wyo. “Then wecame up with idea for this community garden, and it been bigger than we ever expected.” ◦ Road leadinginto Shinnecock Indian reservation is not welcoming one But main purpose of visit – first to reservationby president since Franklin Roosevelt – was simply to pay attention to American Indians, who raked bygrinding poverty Clinton’s own advisers suggested he come up with special proposals geared specifically toIndians’ plight. ◦ “This highlights what going on out there, since beginning of reservation system,” saidSidney Harring, professor at City University of New York School of Law and expert on Indian crime andcriminal law. ◦ American Indians are victims. ◦ President Clinton turned attention to arguably poorest,most forgotten ◦ U.S. citizens: American Indians. ◦ When American Indians began embracing gambling,Hualapai tribe moved quickly to open casino. ◦ members of Northern Arapaho Tribe on Wind RiverReservation started seven-acre community garden with donated land, seeds andFigure 3: MCR Summary for DUC-2006 Topic D0601A, using Trimmer for sentence compression.David Rocchio deputy legal counsel to Vermont Gov. Howard Dean who has been involved in discussionson Indian gambling through the National Governors Association said that the concern that governors haveis not with the benefit casinos bring to tribes ◦ Native Americans living on reservations that maintain 50percent or more unemployment are exempt from the national five year family limit on welfare benefits ◦Smith and thousands like her are seeking help for their substance abuse at the American Indian CommunityHouse the largest of a handful of Native American cultural institutions in the New York area ◦ Juvenilecrime is one strand in the web of social problems facing urban and reservation Indian communities thereport said ◦ Soldierwolf’s family represents the problems that plague many of the 1.3 million AmericanIndians who live on reservations of whom 49 percent are unemployed ◦ Powless said the Onondaga peoplewant to work with the community outside the reservation to improve the economy of the region perhapscreating tourism destinations that might include Indian culture or setting up a free trade zone at unusedmanufacturing sites ◦ As Indian communities across the nation struggle with short funds and a long listof problems they are watching the Navajo Nation’s legal battle with the federal government ◦ recognizeIndians not only Native Americans as Americans ◦ go on reservation system Harring Indian ◦ SupremeCourt allows legalized gambling Indian reservations ◦ American Indian reservations tribal colleges rise fasterthan ◦ main purpose of reservation to pay American Indians by poverty proposalsFigure 4: MCR Summary for DUC-2006 Topic D0601A, using HMM Hedge for sentence compression18Seeking to get a more accurate count of the country’s American Indian population, the Census Bureau isturning to tribal leaders and residents on reservations to help overcome long-standing feelings of warinessor anger toward the federal government. ◦ American Indian reservations would get an infusion of $1.2billion in federal money for education, health care and law enforcement under President Clinton’s proposed2001 budget ◦ Smith and thousands like her are seeking help for their substance abuse at the AmericanIndian Community House, the largest of a handful of Native American cultural institutions in the NewYork area. ◦ Clinton was going to the Pine Ridge Reservation for a visit with the Oglala Sioux nationand to participate in a conference on Native American homeownership and economic development. ◦ saidGlen Revere, a nutritionist with the Indian Health Services on the 2.8 million-acre Wind River Reservation,about 100 miles east of Jackson, Wyo. “Then we came up with the idea for this community garden, andit’s been bigger than we ever expected in so many ways.” ◦ The road leading into the Shinnecock Indianreservation is not a welcoming one ◦ But the main purpose of the visit – the first to a reservation by apresident since Franklin Roosevelt – was simply to pay attention to American Indians, who are so raked bygrinding poverty that Clinton’s own advisers suggested he come up with special proposals geared specificallyto the Indians’ plight. ◦ “This highlights what has been going on out there for 130 years,Figure 5: MCR Summary for DUC-2006 Topic D0601A, with no sentence compression19R1 RecallR1 PrecisionR1 FR2 RecallR2 PrecisionR2 FHMMSentence0.23552(0.230140.24082)0.21896(0.213860.22384)0.22496(0.219830.22978)0.06838(0.065460.07155)0.06287(0.060170.06576)0.06488(0.062090.06785)HMM60 Block0.21381(0.209120.21827)0.18882(0.184440.19301)0.19966(0.195050.20391)0.06133(0.058480.06414)0.05351(0.050970.05588)0.05686(0.054200.05942)TrimmerTopiary0.21014(0.204360.21594)0.20183(0.196270.20722)0.20179(0.196120.20718)0.06337(0.060300.06677)0.06230(0.058870.06617)0.06079(0.057880.06401)0.25143(0.246320.25663)0.23038(0.225670.23522)0.23848(0.233730.24328)0.06637(0.063450.06958)0.06024(0.057470.06326)0.06252(0.059760.06561)Table 2: Rouge scores and 95% confidence intervals for 624 documents from DUC-2003 test set.5System EvaluationsWe tested four single-document summarization systems on the DUC-2003 Task 1 test set:• HMM Hedge using the first sentence of each document (HMM Sentence)• HMM Hedge using the first 60 words of each document (HMM 60 block)• Trimmer• TopiaryTask 1 from DUC-2003 was to construct generic 75-byte summaries for 624 documents drawn from APNewswire and the New York Times. The average size of the documents was 3,997 bytes, so a 75-bytesummary represents a compression ratio of 1.9%.An automatic summarization evaluation tool, Rouge (Lin and Hovy, 2003), was used to evaluatethe results. The system parameters were optimized by hand to maximize the Rouge-1 recall ona comparable training corpus, 500 AP Newswire and New York Times articles from the DUC-2004single-document short summary test data.The Rouge results are shown in Table 2. Results show that HMM Hedge 60 scored significantlylower than most other systems and that Topiary scored higher than all other systems for all R1measures. In addition, HMM Hedge Sentence scored significantly higher than Trimmer for the R1measures.We also evaluated Trimmer and HMM Hedge as components in our Multi-Candidate Reductionframework, along with a baseline that uses the same sentence selector but does not use sentencecompression. All three systems considered the first five sentences of each document and used thesentence selection algorithm presented in Section 4. The feature weights were manually optimized tomaximize Rouge-2 recall on a comparable training corpus, 1,593 Financial Times and Los AngelesTimes articles grouped into 50 topics from the DUC-2005 query-focused multi-document summarization20R1 RecallR2 RecallTrimmerHMM Hedge0.29391(0.285600.30247)0.06718(0.063320.07111)0.27311(0.265540.28008)0.06251(0.058730.06620)NoCompression0.27576(0.267720.28430)0.06126(0.057670.06519)Table 3: Rouge scores and 95% confidence intervals for 50 DUC-2006 test topics, comparing threeMCR variants.MCR ScoreHigherNot DifferentRangeLowerRouge-20.08051230.0678-0.089911Rouge-SU40.13601240.1238-0.147510BE-HM0.04130270.0318-0.05088Table 4: Official DUC-2006 Automatic Metrics for our MCR submission (System 32).test data. The systems were used to generate query-focused, 250-word summaries using the DUC-2006test data, described in Section 4.3.The systems were evaluated using Rouge, configured to omit stopwords from the calculation.7Results are shown in Table 3. MCR using Trimmer compressions scored significantly higher thanMCR using HMM Hedge compressions and the baseline for Rouge-1, but there was not a significantdifference among the three systems for Rouge-2.Finally, the University of Maryland and BBN submitted a version of MCR to the official DUC2006 evaluation. This version used Trimmer as the source of sentence compressions. Results showthat use of sentence compression hurt the system on human evaluation of grammaticality. This is notsurprising, since Trimmer aims to produce compressions that are grammatical in Headlinese, ratherthan standard English. Our MCR run scored significantly lower than 23 systems on NIST’s humanevaluation of grammaticality. However, the system did not score significantly lower than any othersystem on NIST’s human evaluation of content responsiveness. A second NIST evaluation of contentresponsiveness asked evaluators to take readability into consideration. In this evaluation, MCR scoredsignificantly lower than only two systems. The evaluators recognized that Trimmer compressions arenot grammatical in standard English; yet, the content coverage was not significantly different from thebest automatic systems and only two systems were found to be significantly more readable.NIST computed three “official” automatic evaluation metrics for DUC-2006: Rouge-2, RougeSU4 and BE-HM. Table 4 shows the official scores of the submitted MCR system for these threemetrics, along with numbers of systems that scored significantly higher, significantly lower, or were notsignificantly different from our MCR run. Also shown is the range of scores for the systems that werenot significantly different from MCR. These results show that the performance of our MCR run wascomparable to most other systems submitted to DUC-2006.7This is a change in the Rouge configuration from the official DUC-2006 evaluation. We note that the removal ofnon-essential stopwords (typical of Headlinese) is an important component of Trimmer-based sentence compression. Forinternal system comparisons, we configure Rouge in a way that will allow us to detect system differences relevant to ourresearch focus. For reporting of official Rouge results on submitted systems we use the community’s accepted Rougeconfigurations.21The evaluation in Table 3 suggests that Trimmer sentence compression is preferable to HMMHedge sentence compression for generation of English summaries of collections of document in English.However, HMM Hedge may prove to have value with noisier data, as we discuss in the next section.Nevertheless, sentence compression appears to be a valuable component of our framework for multidocument summarization, thus validating the ideas behind Multi-Candidate Reduction.6Applications to Different Types of TextsWe have applied the MCR framework to summarizing different types of texts. In this section we brieflytouch on genre-specific issues that are the subject of ongoing work. Trimmer, Topiary, and HMM Hedgewere designed for summarization of written news. In this genre, the lead sentence is almost alwaysthe first non-trivial sentence of the document. More sophisticated methods for finding lead sentencesdid not outperform the baseline of simply selecting the first sentence for AP wire “hard” news stories.However, some types of articles, such as sports stories, opinion pieces, and movie reviews often donot have informative lead sentences and will require additional work in finding the best sentence forcompression.MCR has also been applied to summarizing transcripts of broadcast news—another input formwhere lead sentences are often not informative. The conventions of broadcast news introduce categoriesof story-initial light content sentences, such as “I’m Dan Rather” or “We have an update on the storywe’ve been following”. These present challenges for the filtering stage of our MCR framework.Such texts are additionally complicated by a range of problems not encountered in written news:noise introduced by automatic speech recognizers or other faulty transcription, issues associated withsentence boundary detection and story boundary detection. If word error rate is high, parser failurescan prevent Trimmer from producing useful output. In this context, HMM Hedge becomes moreattractive, since our language models are more resilient to noisy input.We have performed an initial evaluation of Trimmer, Topiary, and a baseline consisting of thefirst 75 characters of a document, on the task of creating 75-character headlines for broadcast newstranscriptions (Zajic, 2007). The corpus for this task consisted of 560 broadcast news stories fromABC, CNN, NBC, Public Radio International, and Voice of America. We used Rouge-1 recall toevaluate the summaries and found that both systems scored higher than the baseline and that Topiaryscored higher than Trimmer. However there were no significant differences among the systems.Another application of our framework is the summarization of email threads—collections of emailsthat share a common topic or were written as responses to each other. This task can essentially betreated as a multi-document summarization problem, albeit email thread structure introduces someconstraints with respect to the ordering of summary sentences. Noisy data is inherent in this problemand pre-processing to remove quoted text, attachments, and headers is crucial. We have found thatmetadata, such as the name of the sender of each included extract help make email summaries easierto read.We performed an initial evaluation of HMM Hedge and Trimmer as the source of sentence compressions for an email thread summarization system based on the MCR framework (Zajic, 2007). Thecorpus for this task consisted of 10 manually constructed email threads from the Enron Corpus (Klimtand Yang, 2004). We used Rouge-1 and Rouge-2 recall with jackknifing to compare the automaticsystems and the human summarizers. We did not observe a significant difference between the two systems, but we found that the task of summarizing email threads was extremely difficult for the humans(one summarizer scored significantly worse than the automatic systems). This application of MCR toemail thread summarization is an initial effort. The difficulty of the task for the humans suggests thatthe community needs to develop a clearer understanding of what makes a good email thread summaryand to explore practical uses for them.22Finally, Trimmer and HMM Hedge have been applied to Hindi-English cross-language summarization. In this case, Trimmer was applied to the output of machine translation. We adapted HMMHedge to cross-lingual summarization by using the mechanism developed for morphological variationto represent translation probabilities from Hindi story words to English headline words. For moredetails, see Dorr et al. (2003a).7Future WorkFuture work on text summarization under the Multi-Candidate Reduction framework will focus on thethree main components of the architecture: sentence filtering, sentence compression, and candidateselection.For single document summarization, the simple technique of selecting the first non-trivial sentenceof a document for compression remains the best approach. However, for human interest stories or sportsarticles, this approach is less effective. In broadcast news transcripts, the first sentence often does notcontain important information. Currently, filtering for multi-document summarization also relies onthe assumption that important information tends to appear near the front of documents—the first fivesentences of each document are retained to generate compressed candidates. An interesting area offuture work is to explore other approaches to filtering, such as using query relevance and documentcentrality, to move beyond the baseline of selecting the first n sentences. For HMM Hedge, thesemethods can be used to determine the optimal blocks of text on which to apply the decoder.Currently, Trimmer produces multiple compressions by applying rules in a fixed order; the state ofthe compressed sentence after each rule application becomes a candidate. A richer pool of candidatescan be produced by modifying Trimmer rules to operate in order-independent combinations, ratherthan a fixed sequence. We believe that the sentence selector can produce better summaries if it haslarger pools of candidates to choose from. Naturally, different sentence compressions are not the onlytechniques for enriching the candidate pool—other possibilities include merging sentences and resolvinganaphora. Topiary will also be enhanced by using multiple combinations of compressions and topicterms in the context of headline generation.We also plan to enrich the candidate selector by taking into account more features of the currentsummary state. Possibilities include sentence selector iteration count and remaining summary space, aswell as feature weights that change during the progress of summary generation. These extensions willallow us to study the distribution of compressed and uncompressed sentences across sentence selectoriterations. System output can potentially be improved by finer-grained control of this distribution.These features might also help avoid the current problem in which the final sentence is truncated dueto length restrictions (e.g., by selecting a final sentence of more appropriate length).Proper setting of parameters is another important area for future work. Systematic optimizationof parameter values in HMM Hedge and the sentence selector could lead to significant improvementsin output quality. A logical extension to this work would be to learn the best parameter settings, e.g.,through Expectation Maximization.At present, MCR focuses exclusively on summary content selection and does not take sentenceordering into consideration when constructing the summary. Naturally, high-quality summaries shouldread fluently in addition to having relevant content. Recent work in this area that can be appliedto MCR includes includes Conroy et al. (2006), Barzilay et al. (2002), Okazaki et al. (2004), Lapata (2003), and Dorr and Gaasterland (this special issue 2007). Within the MCR architecture, fluencyconsiderations can be balanced with other important factors such as relevance and anti-redundancythrough appropriate feature weighting.238ConclusionThis work presents Multi-Candidate Reduction, a general architecture for multi-document summarization. The framework integrates successful single-document compression techniques that we havepreviously developed. MCR is motivated by the insight that multiple candidate compressions of sourcesentences should be made available to subsequent processing modules, which may have access to moreinformation for summary construction. This is implemented in a dynamic feature-based sentence selector that iteratively builds a summary from compressed variants. Evaluations show that sentencecompression plays an important role in multi-document summarization and that our MCR frameworkis both flexible and extensible.AcknowledgmentsThis work has been supported, in part, under the GALE program of the Defense Advanced ResearchProjects Agency, Contract No. HR0011-06-2-0001, the TIDES program of the Defense AdvancedResearch Projects Agency, BBNT Contract No. 9500006806, and the University of Maryland JointInstitute for Knowledge Discovery. Any opinions, findings, conclusions or recommendations expressedin this paper are those of the authors and do not necessarily reflect the views of DARPA. The firstauthor would like to thank Naomi for proofreading, support, and encouragement. The second authorwould like to thank Steve, Carissa, and Ryan for their energy enablement. The third author would liketo thank Esther and Kiri for their kind support.ReferencesL. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum likelihood approach to continuous speechrecognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.M. Banko, V. Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation.In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL2000), pages 318–325, Hong Kong.R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal of Artificial Intelligence Research, 17:35–55.L. Baum. 1972. An inequality and associated maximization technique in statistical estimation ofprobabilistic functions of a Markov process. Inequalities, 3:1–8.S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor coreferenceresolution for text summarization. In Proceedings of the HLT-NAACL 2003 Text SummarizationWorkshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta.D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. MachineLearning, 34(1/3):211–231.S. Blair-Goldensohn, D. Evans, V.B. Schiffman, A. Schlaikjer, A.at DUC 2004. In Proceedings ofHLT/NAACL 2004, pages 23–30,Hatzivassiloglou, K. McKeown, A. Nenkova, R. Passonneau,Siddharthan, and S. Siegelman. 2004. Columbia Universitythe 2004 Document Understanding Conference (DUC 2004) atBoston, Massachusetts.24P. Brown, J. Cocke, S. Pietra, V. Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin. 1990. Astatistical approach to machine translation. Computational Linguistics, 16(2):79–85.J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reorderingdocuments and producing summaries. In Proceedings of the 21st Annual International ACM SIGIRConference on Research and Development in Information Retrieval (SIGIR 1998), pages 335–336,Melbourne, Australia.Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the First Meetingof the North American Chapter of the Association for Computational Linguistics (NAACL 2000),pages 132–139, Seattle, Washington.J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains,training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for ComputationalLinguistics (COLING/ACL 2006), pages 377–384, Sydney, Australia.J. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document summarization. In Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP2005, Vancouver, Canada.J. Conroy, J. Schlesinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: CLASSY 2006. InProceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006,New York, New York.D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of theThird Conference on Applied Natural Language Processing, Trento, Italy.Hoa Dang and Donna Harman. 2006. Proceedings of the 2006 Document Understanding Conference(DUC 2006) at HLT/NAACL 2006.B. Dorr and T. Gaasterland. this special issue, 2007. Exploiting aspectual features and connectingwords for summarization-inspired temporal-relation extraction. Information Processing and Management.B. Dorr, D. Zajic, and R. Schwartz. 2003a. Cross-language headline generation for Hindi. ACMTransactions on Asian Language Information Processing (TALIP), 2(3):270–289.B. Dorr, D. Zajic, and R. Schwartz. 2003b. Hedge Trimmer: A parse-and-trim approach to headlinegeneration. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and DocumentUnderstanding Conference (DUC 2003), pages 1–8, Edmonton, Alberta.T. Dunning. 1994. Statistical identification of language. Technical Report MCCS 94-273, New MexicoState University.T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13thInternational Workshop on Database and Expert Systems Applications (DEXA 2002), pages 215–222, Aix-en-Provence, France.J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization bysentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summarization, pages 40–48.D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC),Philadelphia.25H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the FirstMeeting of the North American Chapter of the Association for Computational Linguistics (NAACL2000), pages 178–185, Seattle, Washington.B. Klimt and Y. Yang. 2004. Introducing the Enron Corpus. In Proceedings of the First Conferenceon Email and Anti-Spam (CEAS), Mountain View, California.K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. InProceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI-2000), Austin,Texas.K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approachto sentence compression. Artificial Intelligence, 139(1):91–107.M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedingsof the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages545–552, Barcelona, Spain.David Dolan Lewis. 1999. An evaluation of phrasal and clustered representations on a text categorization task. In Proceedings of the 15th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR 1992), pages 37–50, Copenhagen, Denmark.C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Human Language Technology Conference and the North AmericanChapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003),pages 71–78, Edmonton, Alberta.I. Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo.E. Mays, F. Damerau, and R. Mercer. 1990. Context-based spelling correction. In Proceedings of IBMNatural Language ITL, pages 517–522, Paris, France.S. Miller, L. Ramshaw, H. Fox, and R. Weischedel. 2000. A novel use of statistical parsing to extractinformation from text. In Proceedings of the First Meeting of the North American Chapter of theAssociation for Computational Linguistics (NAACL 2000), pages 226–233, Seattle, Washington.S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learningtechniques for email. In Proceedings of the ACL/EACL 2001 Workshop on Computational NaturalLanguage Learning (ConLL), pages 290–297, Toulouse, France.N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving chronological sentence ordering by precedence relation. In Proceedings of the 20th International Conference on Computational Linguistics(COLING 2004), pages 750–756, Geneva, Switzerland.M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. Çelebi, S. Dimitrov, E. Drabek, A. Hakim,W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel, M. Topper, A. Winkel, and Z. Zhang.2004. MEAD—a platform for multidocument multilingual text summarization. In Proceedings ofthe 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon,Portugal.26R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood modelfor topic classification of broadcast news. In Proceedings of the Fifth European Speech Communication Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes,Greece.S. Sista, R. Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discoveryfrom broadcast news stories. In Proceedings of the 2002 Human Language Technology Conference(HLT), pages 99–103, San Diego, California.J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression.In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL2005), pages 290–297, Ann Arbor, Michigan.L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at DUC2006: Task-focusedsummarization with sentence simplification and lexical expansion. In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York.A. Viterbi. 1967. Error bounds for convolution codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260–269.R. Wang, N. Stokes, W. Doran, E. Newman, J. Carthy, and J. Dunnion. 2005. Comparing Topiarystyle approaches to headline generation. In Lecture Notes in Computer Science: Advances inInformation Retrieval: 27th European Conference on IR Research (ECIR 2005), volume 3408,Santiago de Compostela, Spain. Springer Berlin / Heidelberg.D. Zajic, B. Dorr, and R. Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings ofthe 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119,Boston, Massachusetts.D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at MSE2005. In Proceedings of theMSE2005 Track of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures forMT and/or Summarization, Ann Arbor, Michigan.D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J. Lin. 2005b. A sentence-trimming approach tomulti-document summarization. In Proceedings of the 2005 Document Understanding Conference(DUC-2005) at NLT/EMNLP 2005, pages 151–158, Vancouver, Canada.D. Zajic. 2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Summarization Tasks. Ph.D. thesis, University of Maryland, College Park.L. Zhou and E. Hovy. 2003. Headline summarization at ISI. In Proceedings of the HLT-NAACL2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages174–178, Edmonton, Alberta.27</biblio>
</article>
<article>
	<preamble>corpus_train/compression_phrases_Prog-Linear-jair.pdf</preamble>
	<author></author>
	<titre>Journal of Artificial Intelligence Research 31 (2008) 399-429</titre>
	<abstract></abstract>
	<biblio>Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences, 3, 37–56.Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics for generation.In Proceedings of the first International Conference on Natural Language Generation,pp. 1–8, Mitzpe Ramon, Israel.Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning for natural languagegeneration. In Proceedings of the Human Language Technology Conference of theNorth American Chapter of the Association for Computational Linguistics, pp. 359–366, New York, NY, USA.Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.In Proceedings of the 2006 Conference on Empirical Methods in Natural LanguageProcessing, pp. 189–198, Sydney, Australia.Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation of general text. InProceedings of the Third International Conference on Language Resources and Evaluation, pp. 1499–1504, Las Palmas, Gran Canaria.Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st NorthAmerican Annual Meeting of the Association for Computational Linguistics, pp. 132–139, Seattle, WA, USA.Clarke, J., & Lapata, M. (2006). Models for sentence compression: A comparison acrossdomains, training requirements and evaluation measures. In Proceedings of the 21stInternational Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pp. 377–384, Sydney, Australia.Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU–Cambridge toolkit. In Proceedings of Eurospeech’97, pp. 2707–2710, Rhodes, Greece.426Global Inference for Sentence CompressionCormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction to Algorithms. TheMIT Press.Corston-Oliver, S. (2001). Text Compaction for Display on Very Small Screens. In Proceedings of the Workshop on Automatic Summarization at the 2nd Meeting of the NorthAmerican Chapter of the Association for Computational Linguistics, pp. 89–98, Pittsburgh, PA, USA.Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3, 951–991.Dantzig, G. B. (1963). Linear Programming and Extensions. Princeton University Press,Princeton, NJ, USA.Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreferenceresolution using integer programming. In Human Language Technologies 2007: TheConference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pp. 236–243, Rochester, NY.Dras, M. (1999). Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D.thesis, Macquarie University.Galley, M., & McKeown, K. (2007). Lexicalized markov grammars for sentence compression.In In Proceedings of the North American Chapter of the Association for ComputationalLinguistics, pp. 180–187, Rochester, NY, USA.Gomory, R. E. (1960). Solving linear programming problems in integers. In Bellman,R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings of Symposia in AppliedMathematics, Vol. 10, Providence, RI, USA.Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction to Provide anAudio Scanning Service for the Blind. In Hovy, E., & Radev, D. R. (Eds.), Proceedingsof the AAAI Symposium on Intelligent Text Summarization, pp. 111–117, Stanford,CA, USA.Hori, C., & Furui, S. (2004). Speech summarization: an approach through word extractionand a method for evaluation. IEICE Transactions on Information and Systems, E87D (1), 15–25.Jing, H. (2000). Sentence reduction for automatic text summarization. In Proceedings ofthe 6th Applied Natural Language Processing Conference, pp. 310–315, Seattle,WA,USA.Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: a probabilisticapproach to sentence compression. Artificial Intelligence, 139 (1), 91–107.Land, A. H., & Doig, A. G. (1960). An automatic method for solving discrete programmingproblems. Econometrica, 28, 497–520.Lin, C.-Y. (2003). Improving summarization performance by sentence compression — a pilotstudy. In Proceedings of the 6th International Workshop on Information Retrieval withAsian Languages, pp. 1–8, Sapporo, Japan.Lin, D. (2001). LaTaT: Language and text analysis tools. In Proceedings of the first HumanLanguage Technology Conference, pp. 222–227, San Francisco, CA, USA.427Clarke & LapataMarciniak, T., & Strube, M. (2005). Beyond the pipeline: Discrete optimization in NLP. InProceedings of the Ninth Conference on Computational Natural Language Learning,pp. 136–143, Ann Arbor, MI, USA.McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints.In Proceedings of the 11th Conference of the European Chapter of the Association forComputational Linguistics, Trento, Italy.McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation with structured multilabel classification. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pp.987–994, Vancouver, BC, Canada.McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training of dependency parsers. In 43rd Annual Meeting of the Association for ComputationalLinguistics, pp. 91–98, Ann Arbor, MI, USA.Nemhauser, G. L., & Wolsey, L. A. (1988). Integer and Combinatorial Optimization. WileyInterscience series in discrete mathematicals and opitmization. Wiley, New York, NY,USA.Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilisticsentence reduction using support vector machines. In Proceedings of the 20th international conference on Computational Linguistics, pp. 743–749, Geneva, Switzerland.Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). NumericalRecipes in C: The Art of Scientific Computing. Cambridge University Press, NewYork, NY, USA.Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. In Proceedings of the International Conference onComputational Linguistics, pp. 1346–1352, Geneva, Switzerland.Riedel, S., & Clarke, J. (2006). Incremental integer linear programming for non-projectivedependency parsing. In Proceedings of the 2006 Conference on Empirical Methods inNatural Language Processing, pp. 129–137, Sydney, Australia.Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensationusing ambiguity packing and stochastic disambiguation methods for lexical-functionalgrammar. In Human Language Technology Conference and the 3rd Meeting of theNorth American Chapter of the Association for Computational Linguistics, pp. 118–125, Edmonton, Canada.Roark, B. (2001). Probabilistic top-down parsing and language modeling. ComputationalLinguistics, 27 (2), 249–276.Roth, D. (1998). Learning to resolve natural language ambiguities: A unified approach. InIn Proceedings of the 15th of the American Association for Artificial Intelligence, pp.806–813, Madison, WI, USA.Roth, D., & Yih, W. (2004). A linear programming formulation for global inference innatural language tasks. In Proceedings of the Annual Conference on ComputationalNatural Language Learning, pp. 1–8, Boston, MA, USA.428Global Inference for Sentence CompressionRoth, D., & Yih, W. (2005). Integer linear programming inference for conditional randomfields. In Proceedings of the International Conference on Machine Learning, pp. 737–744, Bonn.Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random fields for information extraction. In Advances in Neural Information Processing Systems, Vancouver,BC, Canada.Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. In Proceedings of the 13th International Conference on Computational Linguistics, pp. 253–258,Helsinki, Finland.Turner, J., & Charniak, E. (2005). Supervised and unsupervised learning for sentencecompression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 290–297, Ann Arbor, MI, USA.Vandeghinste, V., & Pan, Y. (2004). Sentence compression for automated subtitling: Ahybrid approach. In Marie-Francine Moens, S. S. (Ed.), Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pp. 89–95, Barcelona, Spain.Williams, H. P. (1999). Model Building in Mathematical Programming (4th edition). Wiley.Winston, W. L., & Venkataramanan, M. (2003). Introduction to Mathematical Programming: Applications and Algorithms (4th edition). Duxbury.Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentencecompression as a tool for document summarization tasks. Information ProcessingManagement Special Issue on Summarization, 43 (6), 1549–1570.429</biblio>
</article>
<article>
	<preamble>corpus_train/hybrid_approach.pdf</preamble>
	<author>Katholieke Universiteit LeuvenMaria Theresiastraat 21BE-3000 LeuvenBelgiumvincent.vandeghinste@ccl.kuleuven.ac.be, yi.pan@ccl.kuleuven.ac.beAbstract</author>
	<titre>Sentence Compression for Automated Subtitling: A Hybrid ApproachVincent Vandeghinste and Yi PanCentre for Computational Linguistics</titre>
	<abstract>AbstractIn this paper a sentence compression tool is described. We describe how an input sentence getsanalysed by using a.o. a tagger, a shallow parserand a subordinate clause detector, and how, basedon this analysis, several compressed versions of thissentence are generated, each with an associated estimated probability. These probabilities were estimated from a parallel transcript/subtitle corpus. Toavoid ungrammatical sentences, the tool also makesuse of a number of rules. The evaluation was doneon three different pronunciation speeds, averagingsentence reduction rates of 40% to 17%. The number of reasonable reductions ranges between 32.9%and 51%, depending on the average estimated pronunciation speed.</abstract>
	<biblio>G. Booij and A. van Santen. 1995. Morfologie. Dewoordstructuur van het Nederlands. AmsterdamUniversity Press, Amsterdam, Netherlands.T. Brants. 2000. TnT - A Statistical Part-of-SpeechTagger. Published online at http://www.coli.unisb.de/thorsten/tnt.W. Daelemans and H. Strik. 2002. Het Nederlands in Taal- en Spraaktechnologie: Prioriteitenvoor Basisvoorzieningen. Technical report, Nederlandse Taalunie.B. Dewulf and G. Saerens. 2000. StijlboekTeletekst Ondertiteling. Technical report, VRT,Brussel. Internal Subtitling Guidelines.W. Haeseryn, G. Geerts, J de Rooij, andM. van den Toorn. 1997. Algemene Nederlandse Spraakkunst. Martinus Nijhoff Uitgevers,Groningen.ITC.1997.Guidance on standardsforsubtitling.Technicalreport,ITC.Online at http://www.itc.org.uk/codes guidelines/broadcasting/tv/sub signaudio/subtitling stnds/.H. Jing. 2001. Cut-and-Paste Text Summarization.Ph.D. thesis, Columbia University.F.J. Koopmans-van Beinum and M.E. van Donzel.1996. Relationship Between Discourse Structureand Dynamic Speech Rate. In Proceedings ICSLP 1996, Philadelphia, USA.N. Oostdijk, W. Goedertier, F. Van Eynde, L. Boves,J.P. Marters, M. Moortgat, and H. Baayen. 2002.Experiences from the Spoken Dutch Corpus. InProceedings of LREC 2002, volume I, pages 340–347, Paris. ELRA.F. Van Eynde. 2004. Part-of-speech Taggingen Lemmatisering. Internal manual of Corpus Gesproken Nederlands, published online athttp://www.ccl.kuleuven.ac.be/Papers/POSmanual febr2004.pdf.V. Vandeghinste and E. Tjong Kim Sang. 2004. Using a parallel transcript/subtitle corpus for sentence compression. In Proceedings of LREC2004, Paris. ELRA.V. Vandeghinste. 2002. Lexicon optimization:Maximizing lexical coverage in speech recognition through automated compounding. In Proceedings of LREC 2002, volume IV, pages 1270–1276, Paris. ELRA.V. Vandeghinste. submitted. ShaRPa: ShallowRule-based Parsing, focused on Dutch. In Proceedings of CLIN 2003.</biblio>
</article>
<article>
	<preamble>corpus_train/marcu_statistics_sentence_pass_one.pdf</preamble>
	<author></author>
	<titre>From: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved.</titre>
	<abstract></abstract>
	<biblio>Barzilay, R.; McKeown, K.; and Elhadad, M. 1999.Information fusion in the context of multi-documentsummarization. In Proceedings of the 37th AnnualMeeting of the Association for Computational Linguistics (ACL–99), 550–557.Berger, A., and Laﬀerty, J. 1999. Information retrievalas statistical translation. In Proceedings of the 22ndConference on Research and Development in Information Retrieval (SIGIR–99), 222–229.Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263–311.Church, K. 1988. A stochastic parts program and nounphrase parser for unrestricted text. In Proceedings ofthe Second Conference on Applied Natural LanguageProcessing, 136–143.Collins, M. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35thAnnual Meeting of the Association for ComputationalLinguistics (ACL–97), 16–23.Grefenstette, G. 1998. Producing intelligent telegraphic text reduction to provide an audio scanningservice for the blind. In Working Notes of the AAAISpring Symposium on Intelligent Text Summarization,111–118.Jelinek, F. 1997. Statistical Methods for Speech Recognition. The MIT Press.Jing, H., and McKeown, K. 1999. The decompositionof human-written summary sentences. In Proceedingsof the 22nd Conference on Research and Developmentin Information Retrieval (SIGIR–99).Knight, K., and Graehl, J. 1998. Machine transliteration. Computational Linguistics 24(4):599–612.Langkilde, I. 2000. Forest-based statistical sentencegeneration. In Proceedings of the 1st Annual Meetingof the North American Chapter of the Association forComputational Linguistics.Linke-Ellis, N. 1999. Closed captioning in America: Looking beyond compliance. In Proceedings ofthe TAO Workshop on TV Closed Captions for thehearing impaired people, 43–59.Magerman, D. 1995. Statistical decision-tree modelsfor parsing. In Proceedings of the 33rd Annual Meetingof the Association for Computational Linguistics, 276–283.Mani, I., and Maybury, M., eds. 1999. Advances inAutomatic Text Summarization. The MIT Press.Mani, I.; Gates, B.; and Bloedorn, E. 1999. Improvingsummaries by revising them. In Proceedings of the 37thAnnual Meeting of the Association for ComputationalLinguistics, 558–565.McKeown, K.; Klavans, J.; Hatzivassiloglou, V.;Barzilay, R.; and Eskin, E. 1999. Towards multidocument summarization by reformulation: Progress andprospects. In Proceedings of the Sixteenth NationalConference on Artiﬁcial Intelligence (AAAI–99).Quinlan, J. 1993. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann Publishers.Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and Burnham, D. 1999. Semi-automatic captioning of TV programs, an Australian perspective. In Proceedings ofthe TAO Workshop on TV Closed Captions for thehearing impaired people, 87–100.Witbrock, M., and Mittal, V.1999.Ultrasummarization: A statistical approach to generatinghighly condensed non-extractive summaries. In Proceedings of the 22nd International Conference on Research and Development in Information Retrieval (SIGIR’99), Poster Session, 315–316.</biblio>
</article>
<article>
	<preamble>corpus_train/mikheev.pdf</preamble>
	<author></author>
	<titre>Periods, Capitalized Words, etc.Andrei Mikheev∗University of Edinburgh</titre>
	<abstract></abstract>
	<biblio>Aberdeen, John S., John D. Burger, David S.Day, Lynette Hirschman, PatriciaRobinson, and Marc Vilain. 1995. “Mitre:Description of the alembic system usedfor MUC-6.” In Proceedings of the SixthMessage Understanding Conference (MUC-6),Columbia, Maryland, November. MorganKaufmann.Baldwin, Breck, Christine Doran, JeffreyReynar, Michael Niv, Bangalore Srinivas,and Mark Wasson. 1997. “EAGLE: Anextensible architecture for generallinguistic engineering.” In Proceedings ofComputer-Assisted Information Searching on316Internet (RIAO ’97), Montreal, June.Baum, Leonard E. and Ted Petrie. 1966.Statistical inference for probabilisticfunctions of finite Markov chains. Annalsof Mathematical Statistics 37:1559–1563.Bikel, Daniel, Scott Miller, RichardSchwartz, and Ralph Weischedel. 1997.“Nymble: A high performance learningname-finder.” In Proceedings of the FifthConference on Applied Natural LanguageProcessing (ANLP’97), pages 194–200.Washington, D.C., Morgan Kaufmann.Brill, Eric. 1995a. Transformation-basederror-driven learning and naturallanguage parsing: A case study inpart-of-speech tagging. ComputationalLinguistics 21(4):543–565.Brill, Eric. 1995b. “Unsupervised learning ofdisambiguation rules for part of speechtagging.” In David Yarovsky and KennethChurch, editors, Proceedings of the ThirdWorkshop on Very Large Corpora, pages1–13, Somerset, New Jersey. Associationfor Computational Linguistics.Burnage, Gavin. 1990. CELEX: A Guide forUsers. Centre for Lexical Information,Nijmegen, Netherlands.MikheevChinchor, Nancy. 1998. “Overview ofMUC-7.” In Seventh Message UnderstandingConference (MUC-7): Proceedings of aConference Held in Fairfax, April. MorganKaufmann.Church, Kenneth. 1988. “A stochastic partsprogram and noun-phrase parser forunrestricted text.” In Proceedings of theSecond ACL Conference on Applied NaturalLanguage Processing (ANLP’88), pages136–143, Austin, Texas.Church, Kenneth. 1995. “One term or two?”In SIGIR’95, Proceedings of the 18th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 310–318, Seattle,Washington, July. ACM Press.Clarkson, Philip and Anthony J. Robinson.1997. “Language model adaptation usingmixtures and an exponentially decayingcache.” In Proceedings IEEE InternationalConference on Speech and Signal Processing,Munich, Germany.Cucerzan, Silviu and David Yarowsky. 1999.“Language independent named entityrecognition combining morphological andcontextual evidence.” In Proceedings ofJoint SIGDAT Conference on EMNLP andVLC.Francis, W. Nelson and Henry Kucera. 1982.Frequency Analysis of English Usage: Lexiconand Grammar. Houghton Mifflin, NewYork.Gale, William, Kenneth Church, and DavidYarowsky. 1992. “One sense perdiscourse.” In Proceedings of the FourthDARPA Speech and Natural LanguageWorkshop, pages 233–237.Grefenstette, Gregory and Pasi Tapanainen.1994. “What is a word, what is asentence? Problems of tokenization.” InThe Proceedings of Third Conference onComputational Lexicography and TextResearch (COMPLEX’94), Budapest,Hungary.Krupka, George R. and Kevin Hausman.1998. Isoquest Inc.: Description of thenetowl extractor system as used forMUC-7. In Proceedings of the SeventhMessage Understanding Conference (MUC-7),Fairfax, VA. Morgan Kaufmann.Kuhn, Roland and Renato de Mori. 1998. Acache-based natural language model forspeech recognition. IEEE Transactions onPattern Analysis and Machine Intelligence12:570–583.Kupiec, Julian. 1992. Robust part-of-speechtagging using a hidden Markov model.Computer Speech and Language.Mani, Inderjeet and T. Richard MacMillan.1995. “Identifying unknown properPeriods, Capitalized Words, etc.names in newswire text.” In B. Boguraevand J. Pustejovsky, editors, CorpusProcessing for Lexical Acquisition. MIT Press,Cambridge, Massachusetts, pages 41–59.Marcus, Mitchell, Mary Ann Marcinkiewicz,and Beatrice Santorini. 1993. Building alarge annotated corpus of English: ThePenn treebank. Computational Linguistics19(2):313–329.Mikheev, Andrei. 1997. Automatic ruleinduction for unknown word guessing.Computational Linguistics 23(3):405–423.Mikheev, Andrei. 1999. A knowledge-freemethod for capitalized worddisambiguation. In Proceedings of the 37thConference of the Association forComputational Linguistics (ACL’99), pages159–168, University of Maryland, CollegePark.Mikheev, Andrei. 2000. “Tagging sentenceboundaries.” In Proceedings of the FirstMeeting of the North American Chapter of theComputational Linguistics (NAACL’2000),pages 264–271, Seattle, Washington.Morgan Kaufmann.Mikheev, Andrei, Clair Grover, and ColinMatheson. 1998. TTT: Text Tokenisation Tool.Language Technology Group, Universityof Edinburgh. Available athttp://www.ltg.ed.ac.uk/software/ttt/index.html.Mikheev, Andrei, Clair Grover, and MarcMoens. 1998. Description of the ltgsystem used for MUC-7. In SeventhMessage Understanding Conference(MUC–7): Proceedings of a Conference Held inFairfax, Virginia. Morgan Kaufmann.Mikheev, Andrei and Liubov Liubushkina.1995. Russian morphology: Anengineering approach. Natural LanguageEngineering 1(3):235–260.Palmer, David D. and Marti A. Hearst. 1994.“Adaptive sentence boundarydisambiguation.” In Proceedings of theFourth ACL Conference on Applied NaturalLanguage Processing (ANLP’94), pages78–83, Stuttgart, Germany, October.Morgan Kaufmann.Palmer, David D. and Marti A. Hearst. 1997.Adaptive multilingual sentence boundarydisambiguation. Computational Linguistics23(2):241–269.Park, Youngja and Roy J. Byrd. 2001.“Hybrid text mining for findingabbreviations and their definitions.” InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMLP’01), pages 16–19, Washington,D.C. Morgan Kaufmann.Ratnaparkhi, Adwait. 1996. “A maximumentropy model for part-of-speech317Computational Linguisticstagging.” In Proceedings of Conference onEmpirical Methods in Natural LanguageProcessing, pages 133–142, University ofPennsylvania, Philadelphia.Reynar, Jeffrey C. and Adwait Ratnaparkhi.1997. “A maximum entropy approach toidentifying sentence boundaries.” InProceedings of the Fifth ACL Conference onApplied Natural Language Processing(ANLP’97), pages 16–19. MorganKaufmann.Riley, Michael D. 1989. “Some applicationsof tree-based modeling to speech and318Volume 28, Number 3language indexing.” In Proceedings of theDARPA Speech and Natural LanguageWorkshop, pages 339–352. MorganKaufmann.Yarowsky, David. 1993. “One sense percollocation.” In Proceedings of ARPAHuman Language Technology Workshop ’93,pages 266–271, Princeton, New Jersey.Yarowsky, David. 1995. “Unsupervisedword sense disambiguation rivalingsupervised methods.” In Meeting of theAssociation for Computational Linguistics(ACL’95), pages 189–196.</biblio>
</article>
<article>
	<preamble>corpus_train/probabilistic_sentence_reduction.pdf</preamble>
	<author>Japan Advanced Institute of Science and Technology1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN{nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jp</author>
	<titre>Probabilistic Sentence Reduction Using Support Vector MachinesMinh Le Nguyen, Akira Shimazu, Susumu HoriguchiBao Tu Ho and Masaru Fukushi</titre>
	<abstract></abstract>
	<biblio>A. Borthwick, “A Maximum Entropy Approachto Named Entity Recognition”, Ph.D thesis, Computer Science Department, New YorkUniversity (1999).C.-C. Chang and C.-J. Lin,“LIBSVM: a library for support vector machines”, Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.H. Jing, “Sentence reduction for automatictext summarization”, In Proceedings of theFirst Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL-2000.T.T. Hastie and R. Tibshirani, “Classificationby pairwise coupling”, The Annals of Statistics, 26(1): pp. 451-471, 1998.C.-W. Hsu and C.-J. Lin, “A comparison ofmethods for multi-class support vector machines”, IEEE Transactions on Neural Networks, 13, pp. 415-425, 2002.K. Knight and D. Marcu, “Summarization beyond sentence extraction: A Probabilistic approach to sentence compression”, ArtificialIntelligence 139: pp. 91-107, 2002.C.Y. Lin, “Improving Summarization Performance by Sentence Compression — A Pilot Study”, Proceedings of the Sixth International Workshop on Information Retrievalwith Asian Languages, pp.1-8, 2003.C. Macleod and R. Grishman, “COMMLEXsyntax Reference Manual”; Proteus Project,New York University (1995).M.L. Nguyen and S. Horiguchi, “A new sentencereduction based on Decision tree model”,Proceedings of 17th Pacific Asia Conferenceon Language, Information and Computation,pp. 290-297, 2003V. Vapnik, “The Natural of Statistical LearningTheory”, New York: Springer-Verlag, 1995.J. Platt,“ Probabilistic outputs for support vector machines and comparison to regularizedlikelihood methods,” in Advances in LargeMargin Classifiers, Cambridege, MA: MITPress, 2000.B. Scholkopf et al, “Comparing Support Vector Machines with Gausian Kernels to RadiusBasis Function Classifers”, IEEE Trans. Signal Procesing, 45, pp. 2758-2765, 1997.</biblio>
</article>
<article>
	<preamble>corpus_train/Stolcke_1996_Automatic_linguistic.pdf</preamble>
	<author></author>
	<titre>AUTOMATIC LINGUISTIC SEGMENTATIONOF CONVERSATIONAL SPEECHAndreas Stolcke</titre>
	<abstract></abstract>
	<biblio>1. A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, 1996.2. E. Brill. Some advances in transformation-based part of speechtagging. In Proceedings of the 12th National Conference onArtificial Intelligence, Seattle, WA, 1994. AAAI Press.3. K. W. Church. A stochastic parts program and noun phraseparser for unrestricted text. In Second Conference on AppliedNatural Language Processing, pages 136–143, Austin, Texas,1988.4. J. J. Godfrey, E. C. Holliman, and J. McDaniel. SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings IEEE Conference on Acoustics, Speechand Signal Processing, volume I, pages 517–520, San Francisco, March 1992.5. F. Jelinek. Self-organized language modeling for speech recognition. In A. Waibel and K.-F. Lee, editors, Readings in SpeechRecognition. Morgan Kaufmann, San Mateo, Ca., 1990.6. S. M. Katz. Estimation of probabilities from sparse data forthe language model component of a speech recognizer. IEEETransactions on Acoustics, Speech, and Signal Processing,35(3):400–401, March 1987.7. M. Meteer et al. Dysfluency annotation stylebook for theSwitchboard corpus. Distributed by LDC, February 1995. Revised June 1995 by Ann Taylor.8. M. Meteer and R. Iyer. Modeling conversational speech forspeech recognition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia,PA, May 1996.9. E. Shriberg and A. Stolcke. Word predictability after hesitations: A corpus-based study. In Proceedings InternationalConference on Spoken Language Processing, Philadelphia, PA,October 1996.10. A. Stolcke and E. Shriberg. Statistical language modelingfor speech disfluencies. In Proceedings IEEE Conference onAcoustics, Speech and Signal Processing, volume I, pages 405–408, Atlanta, GA, May 1996.</biblio>
</article>
<article>
	<preamble>corpus_train/Torres.pdf</preamble>
	<author></author>
	<titre>Summary Evaluationwith and without ReferencesJuan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales</titre>
	<abstract></abstract>
	<biblio>while other measures compare peers with all or some of theinput material:MEASUREM (SUMi,k , Ii0 )3) Update-summarization task that consists of creating asummary out of a cluster of documents and a topic. Twosub-tasks are considered here: A) an initial summary hasto be produced based on an initial set of documents andtopic; B) an update summary has to be produced froma different (but related) cluster assuming documentsused in A) are known. The English TAC’08 UpdateSummarization dataset is used, which consists of 48topics with 20 documents each – 36,911 words.4) Opinion summarization where systems have to analyzea set of blog articles and summarize the opinionsabout a target in the articles. The TAC’08 OpinionSummarization in English4 data set (taken from theBlogs06 Text Collection) is used: 25 clusters and targets(i.e., target entity and questions) were used – 1,167,735words.5) Generic single-document summarization in Spanishusing the Medicina Clı́nica5 corpus, which is composedof 50 medical articles in Spanish, each one with itscorresponding author abstract – 124,929 words.6) Generic single document summarization in French usingthe “Canadien French Sociological Articles” corpusfrom the journal Perspectives interdisciplinaires sur letravail et la santé (PISTES)6 . It contains 50 sociologicalarticles in French, each one with its correspondingauthor abstract – 381,039 words.7) Generic multi-document-summarization in French usingdata from the RPM27 corpus [18], 20 different themesconsisting of 10 articles and 4 abstracts by referencethematic – 185,223 words.(4)where Ii0 is some subset of input Ii . The values producedby the measures for each summary SUMi,k are averagedfor each system k = 0, . . . , s − 1 and these averages areused to produce a ranking. Rankings are then comparedusing Spearman Rank correlation [17] which is used tomeasure the degree of association between two variableswhose values are used to rank objects. We have chosento use this correlation to compare directly results to thosepresented in [12]. Computation of correlations is done usingthe Statistics-RankCorrelation-0.12 package1 , which computesthe rank correlation between two vectors. We also verifiedthe good conformity of the results with the correlation testof Kendall τ calculated with the statistical software R. Thetwo nonparametric tests of Spearman and Kendall do notreally stand out as the treatment of ex-æquo. The goodcorrespondence between the two tests shows that they do notintroduce bias in our analysis. Subsequently will mention onlythe ρ of Sperman more widely used in this field.A. ToolsWe carry out experimentation using a new summarizationevaluation framework: F RESA –FRamework for EvaluatingSummaries Automatically–, which includes document-basedsummary evaluation measures based on probabilitiesdistribution2 . As in the ROUGE package, F RESA supportsdifferent n-grams and skip n-grams probability distributions.The F RESA environment can be used in the evaluation ofsummaries in English, French, Spanish and Catalan, and itintegrates filtering and lemmatization in the treatment ofsummaries and documents. It is developed in Perl and willbe made publicly available. We also use the ROUGE package[10] to compute various ROUGE statistics in new datasets.For experimentation in the TAC and the DUC datasets we usedirectly the peer summaries produced by systems participatingin the evaluations. For experimentation in Spanish and French(single and multi-document summarization) we have createdsummaries at a similar ratio to those of reference using thefollowing systems:– ENERTEX [19], a summarizer based on a theory oftextual energy;– CORTEX [20], a single-document sentence extractionsystem for Spanish and French that combines variousstatistical measures of relevance (angle between sentenceand topic, various Hamming weights for sentences, etc.)and applies an optimal decision algorithm for sentenceselection;– SUMMTERM [21], a terminology-based summarizer thatis used for summarization of medical articles anduses specialized terminology for scoring and rankingsentences;– REG [22], summarization system based on an greedyalgorithm;B. Summarization Tasks and Data SetsWe have conducted our experimentation with the followingsummarization tasks and data sets:1) Generic multi-document-summarization in English(production of a short summary of a cluster of relateddocuments) using data from DUC’043 , task 2: 50clusters, 10 documents each – 294,636 words.2) Focused-based summarization in English (production ofa short focused multi-document summary focused on thequestion “who is X?”, where X is a person’s name) usingdata from the DUC’04 task 5: 50 clusters, 10 documentseach plus a target person name – 284,440 words.4 http://www.nist.gov/tac/data/index.html1 http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/5 http://www.elsevier.es/revistas/ctl2 F RESAis available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/Ressources.html3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.htmlservlet? f=7032&revistaid=26 http://www.pistes.uqam.ca/7 http://www-labs.sinequa.com/rpm215Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales– J S summarizer, a summarization system that scoresand ranks sentences according to their Jensen-Shannondivergence to the source document;– a lead-based summarization system that selects the leadsentences of the document;– a random-based summarization system that selectssentences at random;– Open Text Summarizer [23], a multi-lingual summarizerbased on the frequency and– commercial systems: Word, SSSummarizer8 , Pertinence9and Copernic10 .presented here we used uni-grams, 2-grams, and the skip2-grams with maximum skip distance of 4 (ROUGE-1,ROUGE-2 and ROUGE-SU4). ROUGE is used to comparea peer summary to a set of model summaries in ourframework (as indicated in equation 3).– Jensen-Shannon divergence formula given in Equation 2is implemented in our F RESA package with the followingspecification (Equation 6) for the probability distributionof words w.CTPw = wN(SCwifw∈SNSQw =(6)TCw+δotherwiseN +δ∗BC. Evaluation MeasuresThe following measures derived from human assessment ofthe content of the summaries are used in our experiments:– C OVERAGE is understood as the degree to which onepeer summary conveys the same information as a modelsummary [2]. C OVERAGE was used in DUC evaluations.This measure is used as indicated in equation 3 usinghuman references or models.– R ESPONSIVENESS ranks summaries in a 5-point scaleindicating how well the summary satisfied a giveninformation need [2]. It is used in focused-basedsummarization tasks. This measure is used as indicatedin equation 4 since a human judges the summarywith respect to a given input “user need” (e.g., aquestion). R ESPONSIVENESS was used in DUC and TACevaluations.– P YRAMIDS [11] is a content assessment measure whichcompares content units in a peer summary to weightedcontent units in a set of model summaries. Thismeasure is used as indicated in equation 3 using humanreferences or models. P YRAMIDS is the adopted metricfor content-based evaluation in the TAC evaluations.For DUC and TAC datasets the values of these measures areavailable and we used them directly. We used the followingautomatic evaluation measures in our experiments:– ROUGE [14], which is a recall metric that takes intoaccount n-grams as units of content for comparing peerand model summaries. The ROUGE formula specified in[10] is as follows:PmROUGE-n(R, M ) =P∈ M n−gram∈P countmatch (n − gram)PPcount(n-gram)m ∈MWhere P is the probability distribution of words w intext T and Q is the probability distribution of words win summary S; N is the number of words in text andTsummary N = NT +NS , B = 1.5|V |, Cwis the numberSof words in the text and Cw is the number of words inthe summary. For smoothing the summary’s probabilitieswe have used δ = 0.005. We have also implementedother smoothing approaches (e.g. Good-Turing [24], thatuses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2package11 ) in F RESA, but we do not use them inthe experiments reported here. Following the ROUGEapproach, in addition to word uni-grams we use 2-gramsand skip n-grams computing divergences such as J S(using uni-grams) J S 2 (using 2-grams), J S 4 (using theskip n-grams of ROUGE-SU4), and J S M which is anaverage of the J S i . J Ss measures are used to compare apeer summary to its source document(s) in our framework(as indicated in equation 4). In the case of summarizationof multiple documents, these are concatenated (in thegiven input order) to form a single input from whichprobabilities are computed.IV. E XPERIMENTS AND R ESULTSWe first replicated the experiments presented in [12] toverify that our implementation of J S produced correlationresults compatible with that work. We used the TAC’08Update Summarization data set and computed J S andROUGE measures for each peer summary. We producedtwo system rankings (one for each measure), which werecompared to rankings produced using the manual P YRAMIDSand R ESPONSIVENESS scores. Spearman correlations werecomputed among the different rankings. The results arepresented in Table I. These results confirm a high correlationamong P YRAMIDS, R ESPONSIVENESS and J S. We alsoverified high correlation between J S and ROUGE-2 (0.83Spearman correlation, not shown in the table) in this task anddataset.Then, we experimented with data from DUC’04, TAC’08Opinion Summarization pilot task as well as single and(5)where R is the summary to be evaluated, M is the set ofmodel (human) summaries, countmatch is the number ofcommon n-grams in m and P , and count is the numberof n-grams in the model summaries. For the experiments8 http://www.kryltech.com/summarizer.htm9 http://www.pertinence.net11 http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/10 http://www.copernic.com/en/products/summarizerPolibits (42) 201016Summary Evaluation with and without ReferencesTABLE IS PEARMANMesureROUGE-2JSCORRELATION OF CONTENT- BASED MEASURES INU PDATE S UMMARIZATION TASKP YRAMIDS0.960.85p-valuep < 0.005p < 0.005R ESPONSIVENESS0.920.74evaluation metrics that do not rely on human models but thatcompare summary content to input content directly [12]. Wehave some positive and some negative results regarding thedirect use of the full document in content-based evaluation.We have verified that in both generic muti-documentsummarizationandintopic-basedmulti-documentsummarization in English correlation among measuresthat use human models (P YRAMIDS, R ESPONSIVENESSand ROUGE) and a measure that does not use models(J S divergence) is strong. We have found that correlationamong the same measures is weak for summarization ofbiographical information and summarization of opinions inblogs. We believe that in these cases content-based measuresshould be considered, in addition to the input document, thesummarization task (i.e. text-based representation, description)to better assess the content of the peers [25], the task being adeterminant factor in the selection of content for the summary.Our multi-lingual experiments in generic single-documentsummarization confirm a strong correlation among theJ S divergence and ROUGE measures. It is worth notingthat ROUGE is in general the chosen framework forpresenting content-based evaluation results in non-Englishsummarization.For the experiments in Spanish, we are conscious that weonly have one model summary to compare with the peers.Nevertheless, these models are the corresponding abstractswritten by the authors. As the experiments in [26] show, theprofessionals of a specialized domain (as, for example, themedical domain) adopt similar strategies to summarize theirtexts and they tend to choose roughly the same content chunksfor their summaries. Previous studies have shown that authorabstracts are able to reformulate content with fidelity [27] andthese abstracts are ideal candidates for comparison purposes.Because of this, the summary of the author of a medical articlecan be taken as reference for summaries evaluation. It is worthnoting that there is still debate on the number of models to beused in summarization evaluation [28]. In the French corpusPISTES, we suspect the situation is similar to the Spanishcase.TAC’08p-valuep < 0.005p < 0.005multi-document summarization in Spanish and French. In spiteof the fact that the experiments for French and Spanish corporause less data points (i.e., less summarizers per task) thanfor English, results are still quite significant. For DUC’04,we computed the J S measure for each peer summary intasks 2 and 5 and we used J S, ROUGE, C OVERAGE andR ESPONSIVENESS scores to produce systems’ rankings. Thevarious Spearman’s rank correlation values for DUC’04 arepresented in Tables II (for task 2) and III (for task 5).For task 2, we have verified a strong correlation betweenJ S and C OVERAGE. For task 5, the correlation betweenJ S and C OVERAGE is weak, and that between J S andR ESPONSIVENESS is weak and negative.Although the Opinion Summarization (OS) task is a newtype of summarization task and its evaluation is a complicatedissue, we have decided to compare J S rankings with thoseobtained using P YRAMIDS and R ESPONSIVENESS in TAC’08.Spearman’s correlation values are listed in Table IV. As it canbe seen, there is weak and negative correlation of J S withboth P YRAMIDS and R ESPONSIVENESS. Correlation betweenP YRAMIDS and R ESPONSIVENESS rankings is high for thistask (0.71 Spearman’s correlation value).For experimentation in mono-document summarizationin Spanish and French, we have run 11 multi-lingualsummarization systems; for experimentation in French, wehave run 12 systems. In both cases, we have producedsummaries at a compression rate close to the compression rateof the authors’ provided abstracts. We have then computed J Sand ROUGE measures for each summary and we have averagedthe measure’s values for each system. These averages wereused to produce rankings per each measure. We computedSpearman’s correlations for all pairs of rankings.Results are presented in Tables V, VI and VII. All resultsshow medium to strong correlation between the J S measuresand ROUGE measures. However the J S measure based onuni-grams has lower correlation than J Ss which use n-gramsof higher order. Note that table VII presents results forgeneric multi-document summarization in French, in thiscase correlation scores are lower than correlation scores forsingle-document summarization in French, a result which maybe expected given the diversity of input in multi-documentsummarization.VI. C ONCLUSIONS AND F UTURE W ORKThis paper has presented a series of experiments incontent-based measures that do not rely on the use of modelsummaries for comparison purposes. We have carried outextensive experimentation with different summarization tasksdrawing a clearer picture of tasks where the measures couldbe applied. This paper makes the following contributions:– We have shown that if we are only interested in rankingsummarization systems according to the content of theirautomatic summaries, there are tasks were models couldbe subtituted by the full document in the computation ofthe J S measure obtaining reliable rankings. However,we have also found that the substitution of modelsby full-documents is not always advisable. We haveV. D ISCUSSIONThe departing point for our inquiry into text summarizationevaluation has been recent work on the use of content-based17Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-MoralesTABLE IIS PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2MesureROUGE-2JSC OVERAGE0.790.68p-valuep < 0.0050p < 0.0025TABLE IIIS PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5MesureROUGE-2JSC OVERAGE0.780.40p-valuep < 0.001p < 0.050R ESPONSIVENESS0.44-0.18p-valuep < 0.05p < 0.25TABLE IVS PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OSMesureJSP YRAMIDS-0.13p-valuep < 0.25R ESPONSIVENESS-0.14TASKp-valuep < 0.25TABLE VS PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH )MesureJSJ S2J S4J SMROUGE -10.560.880.880.82p-valuep < 0.100p < 0.001p < 0.001p < 0.005ROUGE -20.460.800.800.71ROUGE -SU40.450.810.810.71p-valuep < 0.200p < 0.005p < 0.005p < 0.010a representation of the task/topic in the calculation ofmeasures. To carry out these comparisons, however, we aredependent on the existence of references.F RESA will also be used in the new question-answer taskcampaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/qa.asp) for the evaluation of long answers. This task aimsto answer a question by extraction and agglomeration ofsentences in Wikipedia. This kind of task correspondsto those for which we have found a high correlationamong the measures J S and evaluation methods withhuman intervention. Moreover, the J S calculation will beamong the summaries produced and a representative set ofrelevant passages from Wikipedia. F RESA will be used tocompare three types of systems, although different tasks: themulti-document summarizer guided by a query, the searchsystems targeted information (focused IR) and the questionanswering systems.found weak correlation among different rankings incomplex summarization tasks such as the summarizationof biographical information and the summarization ofopinions.– We have also carried out large-scale experiments inSpanish and French which show positive medium tostrong correlation among system’s ranks produced byROUGE and divergence measures that do not use themodel summaries.– We have also presented a new framework, F RESA, forthe computation of measures based on J S divergence.Following the ROUGE approach, F RESA package useword uni-grams, 2-grams and skip n-grams computingdivergences. This framework will be available to thecommunity for research purposes.Although we have made a number of contributions, this paperleaves many open questions than need to be addressed. Inorder to verify correlation between ROUGE and J S, in theshort term we intend to extend our investigation to otherlanguages such as Portuguese and Chinesse for which wehave access to data and summarization technology. We alsoplan to apply F RESA to the rest of the DUC and TACsummarization tasks, by using several smoothing techniques.As a novel idea, we contemplate the possibility of adaptingthe evaluation framework for the phrase compression task[29], which, to our knowledge, does not have an efficientevaluation measure. The main idea is to calculate J S froman automatically-compressed sentence taking the completesentence by reference. In the long term, we plan to incorporatePolibits (42) 2010p-valuep < 0.100p < 0.002p < 0.002p < 0.020ACKNOWLEDGMENTWe are grateful to the Programa Ramón y Cajal fromMinisterio de Ciencia e Innovación, Spain. This work ispartially supported by: a postdoctoral grant from the NationalProgram for Mobility of Research Human Resources (NationalPlan of Scientific Research, Development and Innovation2008-2011, Ministerio de Ciencia e Innovación, Spain); theresearch project CONACyT, number 82050, and the researchproject PAPIIT-DGAPA (Universidad Nacional Autónoma deMéxico), number IN403108.18Summary Evaluation with and without ReferencesTABLE VIS PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE PISTES C ORPUS (F RENCH )MesureJSJ S2J S4J SMROUGE -10.700.930.830.88p-valuep < 0.050p < 0.002p < 0.020p < 0.010ROUGE -20.730.860.760.83p-valuep < 0.05p < 0.01p < 0.05p < 0.02ROUGE -SU40.730.860.760.83p-valuep < 0.500p < 0.005p < 0.050p < 0.010TABLE VIIS PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE RPM2 C ORPUS (F RENCH )MeasureJSJ S2J S4J SMROUGE -10.8300.8000.7500.850p-valuep < 0.002p < 0.005p < 0.010p < 0.002ROUGE -20.6600.5900.5200.640R EFERENCESp-valuep < 0.05p < 0.05p < 0.10p < 0.05ROUGE -SU40.7410.6800.6200.740p-valuep < 0.01p < 0.02p < 0.05p < 0.01[18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,“A French Human Reference Corpus for multi-documentssummarization and sentence compression,” in LREC’10, vol. 2,Malta, 2010, p. In press.[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energyof Associative Memories: performants applications of Enertex algorithmin text summarization and topic segmentation,” in MICAI’07, 2007, pp.861–871.[20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,“Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,St Malo, France, 2002, pp. 723–734.[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,“Automatic summarization using terminological and semanticresources,” in LREC’10, vol. 2, Malta, 2010, p. In press.[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme gloutonappliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,p. In press.[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modernsystems of automatic text summarization,” Automatic Documentationand Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.[24] C. D. Manning and H. Schütze, Foundations of Statistical NaturalLanguage Processing.Cambridge, Massachusetts: The MIT Press,1999.[25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,vol. 43, no. 6, pp. 1449–1481, 2007.[26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specializeddiscourse: The case of medical articles in spanish,” Terminology, vol. 13,no. 2, pp. 249–286, 2007.[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACLStudent Research Workshop.Toulouse, France: Association forComputational Linguistics, 9-11 July 2001 2001, pp. 49–54.[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,Singapore, August 2009, pp. 23–30.[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:Sentence compression,” in Proceedings of the National Conference onArtificial Intelligence. Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999, 2000, pp. 703–710.[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andB. Sundheim, “Summac: a text summarization evaluation,” NaturalLanguage Engineering, vol. 8, no. 1, pp. 43–68, 2002.[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,no. 6, pp. 1506–1520, 2007.[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,USA: NIST, November 17-19 2008.[4] K. Spärck Jones and J. Galliers, Evaluating Natural LanguageProcessing Systems, An Analysis and Review, ser. Lecture Notes inComputer Science. Springer, 1996, vol. 1083.[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison ofrankings produced by summarization evaluation measures,” in NAACLWorkshop on Automatic Summarization, 2000, pp. 69–78.[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluationof Summaries in a Cross-lingual Environment using Content-basedMetrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,D. Liu, and E. Drábek, “Evaluation challenges in large-scale documentsummarization,” in ACL’03, 2003, pp. 375–382.[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a methodfor automatic evaluation of machine translation,” in ACL’02, 2002, pp.311–318.[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in EvaluationInitiatives in Natural Language Processing. Budapest, Hungary: EACL,14 April 2003.[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation ofSummaries,” in Text Summarization Branches Out: ACL-04 Workshop,M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection inSummarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.145–152.[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selectionin Summarization without Human Models,” in Empirical Methods inNatural Language Processing, Singapore, August 2009, pp. 306–314.[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEETransactions on Information Theory, vol. 37, no. 145-151, 1991.[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries UsingN-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,USA: Association for Computational Linguistics, 2003, pp. 71–78.[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoreticapproach to automatic evaluation of summaries,” in HLT-NAACL,Morristown, USA, 2006, pp. 463–470.[16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. ofMath. Stat., vol. 22, no. 1, pp. 79–86, 1951.[17] S. Siegel and N. Castellan, Nonparametric Statistics for the BehavioralSciences. McGraw-Hill, 1998.19Polibits (42) 2010</biblio>
</article>
