<article>
	<preamble>compression.pdf</preamble>
	<author>1</author>
	<titre>Multi-Candidate Reduction: Sentence Compression as a Tool forDocument Summarization Tasks∗David Zajic1 , Bonnie J. Dorr1 , Jimmy Lin1 , Richard Schwartz2</titre>
	<abstract></abstract>
	<biblio>the same source sentences are available as candidates for inclusion in the final summary.Minimization of redundancy is an important element of a multi-document summarization system.Carbonell and Goldstein (1998) propose a technique called Maximal Marginal Relevance (MMR) forranking documents returned by an information retrieval system so that the front of the ranked list willcontain diversity as well as high relevance. Goldstein et al. (2000) extend MMR to multi-documentsummarization. MCR borrows the ranking approach of MMR, but uses a different set of features. LikeMEAD, these approaches use feature weights that are optimized to maximize an automatic metric ontraining data.Several researchers have shown the importance of summarization in domains other than writtennews (Muresan et al., 2001; Clarke and Lapata, 2006). Within the MCR framework, we discuss theportability of Trimmer and HMM Hedge to a variety of different texts: written news, broadcast newstranscriptions, email threads, and text in foreign language.43Single-Sentence CompressionOur general approach to the generation of a summary from a single document is to produce a headlineby selecting words in order from the text of the story. Consider the following excerpt from a news storyand corresponding headline:(1)(i)(ii)After months of debate following the Sept. 11 terrorist hijackings, the TransportationDepartment has decided that airline pilots will not be allowed to have guns in thecockpits.Pilots not allowed to have guns in cockpits.The bold words in (1i) form a fluent and accurate headline, as shown in (1ii).This basic approach has been realized in two ways. The first, Trimmer, uses a linguisticallymotivated algorithm to remove grammatical constituents from the lead sentence until a length thresholdis met. Topiary is a variant of Trimmer that combines fluent text from a compressed sentence withtopic terms to produce headlines. The second, HMM Hedge, employs a noisy-channel model to findthe most likely headline for a given story. The remainder of this section will present Trimmer, Topiary,and HMM Hedge in more detail.3.1TrimmerOur first approach to sentence compression involves iteratively removing grammatical constituents fromthe parse tree of a sentence using linguistically-motivated rules until a length threshold has been met.When applied to the lead sentence, or first non-trivial sentence of a story, our algorithm generates avery short summary, or headline. This idea is implemented in our Trimmer system, which can leveragethe output of any constituency parser that uses the Penn Treebank conventions. At present we useCharniak’s parser (Charniak, 2000).The insights that form the basis and justification for the Trimmer rules come from our previousstudy, which compared the relative prevalence of certain constructions in human-written summariesand lead sentences in stories. This study used 218 human-written summaries of 73 documents fromthe TIPSTER corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries andthe lead sentences of the 73 stories were parsed using the BBN SIFT parser (Miller et al., 2000). Theparser produced 957 noun phrases (NP nodes in the parse trees) and 315 clauses (S nodes in the parsetrees) for the 218 summaries. For the 73 lead sentences, the parser produced 817 noun phrases and316 clauses.At each level (sentence, clause, and noun phrase), different types of linguistic phenomena werecounted.• At the sentence level, the numbers of preposed adjuncts, conjoined clauses, and conjoined verbphrases were counted. Children of the root S node that occur to the left of the first NP areconsidered to be preposed adjuncts. The bracketed phase in “[According to police] the crime ratehas gone down” is a prototypical example of a preposed adjunct.• At the clause level, temporal expressions, trailing SBAR nodes, and trailing PP nodes werecounted. Trailing constituents are those not designated as an argument of a verb phrase.• At both the sentence and clause levels, conjoined S nodes and conjoined VP nodes were counted.• At the NP level, determiners and relative clauses were counted.The counts and prevalence of the phenomena in the human-generated headlines and lead sentencesare shown in Table 1. The results of this analysis illuminated the opportunities for trimming constituents and guided the development of our Trimmer rules, detailed below.5LevelSentenceClauseNoun PhrasePhenomenonpreposed adjunctsconjoined Sconjoined VPtemporal expressiontrailing PPtrailing SBARrelative clausedeterminerSummary0/2180%1/2180.5%7/2183%5/3151.5%165/315 52%24/3158%3/9570.3%31/9573%Lead Sentence2/732.7%3/734%20/7327%77/31624%184/316 58%49/31616%29/8173.5%205/817 25%Table 1: Counts and prevalence of phenomena found in summaries and lead sentences.3.1.1Trimmer AlgorithmTrimmer applies syntactic compression rules to a parse tree according the following algorithm:1. Remove temporal expressions2. Select Root S node3. Remove preposed adjuncts4. Remove some determiners5. Remove conjunctions6. Remove modal verbs7. Remove complementizer that8. Apply the XP over XP rule9. Remove PPs that do not contain named entities10. Remove all PPs under SBARs11. Remove SBARSs12. Backtrack to state before Step 913. Remove SBARs14. Remove PPs that do not contain named entities15. Remove all PPsSteps 1 and 4 of the algorithm remove low-content units from the parse tree.Temporal expressions—although certainly not content-free—are not usually vital for summarizingthe content of an article. Since the goal is to provide an informative headline, the identification andelimination of temporal expressions (Step 1) allow other more important details to remain in the lengthconstrained headline. The use of BBN’s IdentiFinderTM (Bikel et al., 1999) for removal of temporalexpressions is described in Section 3.1.2.The determiner rule (Step 4) removes leaf nodes that are assigned the part-of-speech tag DT andhave the surface form the, a, or an. The intuition for this rule is that the information carried by6articles is expendable in summaries, even though this makes the summaries ungrammatical for generalEnglish. Omitting articles is one of the most salient features of newspaper headlines. Sentences (2)and (3), taken from the New York Times website on September 27, 2006, illustrate this phenomenon.The italicized articles did not occur in the actual newspaper headlines.(2)The Gotti Case Ends With a Mistrial for the Third Time in a Year(3)A Texas Case Involving Marital Counseling Is the Latest to Test the Line Between Church andStateStep 2 identifies nodes in the parse tree of a sentence that could serve as the root of a compressionfor the sentence. Such nodes will be referred to as Root S nodes. A node in a tree is a Root S node if itis labeled S in the parse tree and has children labeled NP and VP, in that order. The human-generatedheadlines we studied always conform to this rule. It has been adopted as a constraint in the Trimmeralgorithm that the lowest leftmost Root S node is taken to be the root node of the headline. Anexample of this rule application is shown in (4). The boldfaced material in the parse is retained andthe italicized material is eliminated.(4)(i)Input: Rebels agreed to talks with government officials, international observers said Tuesday.(ii)Parse: [S [S [NP Rebels][VP agreed to talks with government officials]], international observers said Tuesday.](iii) Output: Rebels agreed to talks with government officials.When the parser produces a usable parse tree, this rule selects a valid starting point for compression.However, the parser sometimes produces incorrect output, as in the cases below (from DUC-2003):(5)(i)Parse: [S[SBAR What started as a local controversy][VP has evolved into aninternational scandal.]](ii)Parse: [NP[NP Bangladesh][CC and][NP[NP India][VP signed a water sharingaccord.]]]In (5i), an S exists, but it does not conform to the requirements of the Root S rule because it doesnot have as children an NP followed by a VP. The problem is resolved by selecting the lowest leftmostS, ignoring the constraints on the children. In (5ii), no S is present in the parse. This problem isresolved by selecting the root of the entire parse tree as the root of the headline. These parsing errorsoccur infrequently—only 6% of the sentences in the DUC-2003 evaluation data exhibit these problems,based on parses generated by the BBN SIFT parser.The motivation for removing preposed adjuncts (Step 3) is that all of the human-generated headlinesomit what we refer to as the preamble of the sentence. Preposed adjuncts are constituents that precedethe first NP (the subject) under the Root S chosen in Step 2; the preamble of a sentence consists of itspreposed adjuncts. The impact of preposed adjunct removal can be seen in example (6).(6)(i)Input: According to a now finalized blueprint described by U.S. officials and other sources,the Bush administration plans to take complete, unilateral control of a post-Saddam HusseinIraq.(ii)Parse: [S[PP According to a now finalized blueprint described by U.S. officials and othersources], [Det the] Bush administration plans to take complete, unilateral controlof[Det a] post-Saddam Hussein Iraq.]7(iii) Output: Bush administration plans to take complete unilateral control of post-SaddamHussein Iraq.The remaining steps of the algorithm remove linguistically peripheral material through successivedeletions of constituents until the sentence is shorter than a length threshold. Each stage of thealgorithm corresponds to the application of one of the rules. Trimmer first finds the pool of nodes inthe parse to which a rule can be applied. The rule is then iteratively applied to the deepest, rightmostremaining node in the pool until the length threshold is reached or the pool is exhausted. After a rulehas been applied at all possible nodes in the parse tree, the algorithm moves to the next step.In the case of a conjunction with two children (Step 5), one of the children will be removed. If theconjunction is and , the second child is removed. If the conjunction is but, the first child is removed.This rule is illustrated by the following examples, where the italicized text is trimmed.(7)When Sotheby’s sold a Harry S Truman signature that turned out to be a reproduction, theprestigious auction house apologized and bought it back .(8)President Clinton expressed sympathy after a car-bomb explosion in a Jerusalem market wounded24 people but said the attack should not derail the recent land-for-security deal between Israeland the Palestinians.The modal verb rule (Step 6) applies to verb phrases in which the head is a modal verb and thehead of the child verb phrase is a form of have or be. In such cases, the modal and auxiliary verbsare removed. Sentences (9) and (10) show examples of this rule application. Note that although inSentence (10) the omission of trimmed material changes the meaning, given a tight space constraint,the loss of the modality is preferable to the loss of other content information.(9)People’s palms and fingerprints may be used to diagnose schizophrenia.(10) Agents may have fired potentially flammable tear gas cannisters.The complementizer rule (Step 7) removes the word that when it occurs as a complementizer.Sentence (11) shows an example in which two complementizers can be removed.(11) Hoffman stressed that study is only preliminary and can’t prove that treatment useful.The XP-over-XP rule (Step 8) is a linguistic generalization that allows a single rule to cover twodifferent phenomena. XP in the name of the rule is a variable that can take two values: NP and VP.In constructions of the form [XP [XP ...] ...], the other children of the higher XP are removed. Notethat the child XP must be the first child of the parent XP. When XP = NP the rule removes relativeclauses (as in Sentence (12)) and appositives (as in Sentence (13)).(12) Schizophrenia patients whose medication couldn’t stop the imaginary voices in their heads gainedsome relief.(13) A team led by Dr. Linda Brzustowicz, assistant professor of neuroscience at Rutgers University’sCenter for Molecular and Behavioral Neuroscience in Newark, studied DNA of dozens of membersof 22 families.The rules that remove prepositional phrases and subordinate clauses (Steps 9 through 15) aresometimes prone to removing important content. Thus, these rules are applied last, only when thereare no other types of rules to apply. Moreover, these rules are applied with a backoff option to avoid8over-trimming the parse tree. First, the PP rule is applied (Steps 9 and 10),1 followed by the SBARrule (Step 11). If the desired sentence length has not been reached, the system reverts to the parse treeas it was before any prepositional phrases were removed (Step 12) and applies the SBAR rule (Step13). If the desired length still has not been reached, the PP rule is applied again (Steps 14 and 15).The intuition behind this ordering is that, when removing constituents from a parse tree, it is preferable to remove smaller fragments before larger ones and prepositional phrases tend to be smaller thansubordinate clauses. Thus, Trimmer first attempts to achieve the desired length by removing smallerconstituents (PPs), but if this cannot be accomplished, the system restores the smaller constituents,removes a larger constituent, and then resumes the deletion of the smaller constituents. To reduce therisk of removing prepositional phrases that contain important information, BBN’s IdentiFinder is usedto distinguish PPs containing temporal expressions and named entities, as described next.3.1.2Use of BBN’s IdentiFinder in TrimmerBBN’s IdentiFinder is used both for the elimination of temporal expressions and for conservativedeletion of PPs containing named entities. The elimination of temporal expressions (Step 1) is a twostep process: (a) use IdentiFinder to mark temporal expressions; and (b) remove [PP ... [NP [X] ...]...] and [NP [X]] where X is tagged as part of a temporal expression. The following examples illustratethe application of temporal expression removal rule:(14) (i)(ii)Input: The State Department on Friday lifted the ban it had imposed on foreign fliers.Parse: [S [NP[Det The] State Department [PP [IN on] [NP [NNP Friday]]] [VP lifted[Det the] ban it had imposed on foreign fliers.]](iii) Output: State Department lifted ban it had imposed on foreign fliers.(15) (i)Input: An international relief agency announced Wednesday that it is withdrawing fromNorth Korea.(ii) Parse: [S [NP [Det An] international relief agency][VP announced [NP [NNP Wednesday]] that it is withdrawing from North Korea.]](iii) Output: International relief agency announced that it is withdrawing from North Korea.IdentiFinder is also used to ensure that prepositional phrases containing named entities are notremoved during the first round of PP removal (Step 9). However, prepositional phrases containingnamed entities that are descendants of SBARs are removed before the parent SBAR is removed, sincewe should remove a smaller constituent before removing a larger constituent that subsumes it. Sentence(16) shows an example of a SBAR subsuming two PPs, one of which contains a named entity.(16) The commercial fishing restrictions in Washington will not be lifted [SBAR unless the salmonpopulation increases [PP to a sustainable number] [PP in the Columbia River]].If the PP rule were not sensitive to named entities, the PP in the Columbia River would be thefirst prepositional phrase to be removed, because it is the lowest rightmost PP in the parse. However,this PP provides an important piece of information: the location of the salmon population. The rule inStep 9 will skip the last prepositional phrase and remove the penultimate PP to a sustainable number .This concludes an overview of the Trimmer rules and our syntactic sentence compression algorithm.Given a length limit, the system will produce a single compressed version of the target sentence.Multiple compressions can be generated by setting the length limit to be very small and storing the stateof the sentence after each rule application as a compressed variant. In section 4, we will describe howmultiple compressed candidates generated by Trimmer are used as a component of a multi-documentsummarization system.1The reason for breaking PP removal into two stages is discussed in Section 3.1.2.93.2TopiaryWe have used the Trimmer approach to compression in another variant of single-sentence summarizationcalled Topiary. This system combines Trimmer with a topic discovery approach (described next) toproduce a fluent summary along with additional context.The Trimmer algorithm is constrained to build a headline from a single sentence. However, it isoften the case that no single sentence contains all the important information in a story. Relevantinformation can be spread over multiple sentences, linked by anaphora or ellipsis. In addition, thechoice of lead sentence may not be ideal and our trimming rules are imperfect.On the other hand, approaches that construct headlines from lists of topic terms (Lewis, 1999;Schwartz et al., 1997) also have limitations. For example, Unsupervised Topic Discovery (UTD)—described below—rarely generates any topic terms that are verbs. Thus, topic lists are good at indicating the general subject but rarely give any direct indication of what events took place. Intuitively,we need both fluent text to tell what happened and topic terms to provide context.3.2.1Topic Term Generation: UTD and OnTopicOnTopic (Schwartz et al., 1997) uses an HMM to assign topics to a document; topic models are derivedfrom an annotated corpus. However, it is often difficult to acquire such data, especially for a new genreor language. UTD (Sista et al., 2002) was developed to overcome this limitation: it takes as input alarge unannotated corpus and automatically creates a set of topic models with meaningful names.The UTD algorithm has several stages. First, it analyzes the corpus to find multi-word sequencesthat can be treated as single tokens. It does this using two methods. One method is a minimumdescription length criterion that detects phrases that occur frequently relative to the individual words.The second method uses BBN’s IdentiFinder to detect multi-word names. These names are added tothe text as additional tokens. They are also likely to be chosen as potential topic names. In the secondstage of UTD, we find those terms (both single-word and multi-word) with high tf.idf. Only thosetopic names that occur as high-content terms in at least four different documents are kept. The thirdstage trains topic models corresponding to these topic terms. The modified Expectation Maximizationprocedure of BBN’s OnTopic system is used to determine which words in the documents often signifythese topic names. This produces topic models. Fourth, these topic models are used to find the mostlikely topics for each document, which is equivalent to assigning the name of the topic model to thedocument as a topic term. This often assigns topics to documents where the topic name does not occurin the document text.We found, in various experiments (Sista et al., 2002), that the topic names derived by this procedurewere usually meaningful and that the topic assignment was about as good as when the topics werederived from a corpus that was annotated by people. We have also used this procedure on differentlanguages and shown the same behavior. Since UTD is unsupervised, it can run equally well on a newlanguage, as long as the documents can be divided into strings that approximate words.The topic list in (17) was generated by UTD and OnTopic for a story about the FBI investigationof the 1998 bombing of the U.S. embassy in Nairobi.(17) BIN LADEN, EMBASSY, BOMBING, POLICE OFFICIALS, PRISON, HOUSE, FIRE, KABILATopiary uses UTD to generate topic terms for the collection of documents to be summarized anduses OnTopic to assign the topic terms to the documents. The next section will describe how topicterms and sentence compressions are combined to form Topiary summaries.103.2.2Topiary AlgorithmAs each Trimmer rule is applied to a sentence, the resulting state of the sentence is stored as acompressed variant of the source sentence. Topiary selects from the variants the longest one suchthat there is room to prepend the highest scoring non-redundant topic term. Suppose the highestscoring topic term is “terrorism” and the length threshold is 75 characters. To make room for the topic“terrorism”, the length threshold is lowered by 10 characters: 9 characters for the topic and 1 characteras a separator. Thus, Topiary chooses the longest trimmed variant under 65 characters that does notcontain the word “terrorism”, If there is no such candidate, i.e., all the trimmed variants contain theword terrorism, Topiary would consider the second highest scoring topic word, “bomb”. Topiary wouldselect the longest trimmed variant under 70 characters that does not contain the word “bomb”. AfterTopiary has selected a trimmed variant and prepended a topic to it, it checks to see how much unusedspace remains under the threshold. Additional topic words are added between the first topic word andthe compressed sentence until all space is exhausted.This process results in a headline that contains one or more main topics about the story and ashort sentence that says what happened concerning them. The combination is often more concise thana fully fluent sentence and compensates for the fact that the information content from the topic andthe compressed sentence do not occur together in any single sentence from the source text.As examples, sentences (18) and (19) are the outputs of Trimmer and Topiary, respectively, for thesame story in which UTD selected the topic terms in (17).(18) FBI agents this week began questioning relatives of the victims(19) BIN LADEN, EMBASSY, BOMBING: FBI agents this week began questioning relativesBy combining topics and parse-and-trim compression, Topiary achieved the highest score on thesingle-document summarization task (i.e., headline generation task) in DUC-2004 (Zajic et al., 2004).3.3HMM HedgeOur second approach to sentence compression, implemented in HMM Hedge, treats the observed data(the story) as the result of unobserved data (headlines) that have been distorted by transmissionthrough a noisy channel. The effect of the noisy channel is to add story words between the headlinewords. The model is biased by parameters to make the resulting headlines more like Headlinese, theobserved language of newspaper headlines created by copy editors.Formally, we consider a story S to be a sequence of N words. We want to find a headline H, asubsequence of words from S, that maximizes the likelihood that H generated the story S, or:argmaxH P (H|S)It is difficult to directly estimate P (H|S), but this probability can be expressed in terms of otherprobabilities that are easier to compute, using Bayes’ rule:P (H|S) =P (S|H)P (H)P (S)Since the goal is to maximize this expression over H, and P (S) is constant with respect to H, thedenominator of the above expression can be omitted. Thus we wish to find:argmaxH P (S|H)P (H)Let H be a headline consisting of words h1 , h2 , ..., hn . Let the special symbols start and end representthe beginning and end of a headline, respectively. P (H) can be estimated using a bigram model ofHeadlinese:P (H) = P (h1 |start)P (h2 |h1 )...P (end|hn )11The bigram probabilities of the words in the headline language were computed from a corpus ofEnglish headlines taken from 242,918 AP newswire stories in the TIPSTER corpus. The headlinescontain 2,848,194 words from a vocabulary of 88,627 distinct words.Given a story S and a headline H, the action of the noisy channel is to form S by adding non-headlinewords to H. Let G be the non-headline words added by the channel to the headline: g1 , g2 , ..., gm . Forthe moment, we assume that the headline words are transmitted through the channel with probability1. We estimate P (S|H), the probability that the channel added non-headline words G to headline Hto form story S. This is accomplished using a unigram model of newspaper stories that we will referto as the general language, in contrast to the headline language. Let Pgl (g) be the probability ofnon-headline word g in the general language and Pch (h) = 1 be the probability that headline word his transmitted through the channel as story word h.P (S|H) = Pgl (g1 )Pgl (g2 )...Pgl (gm )Pch (h1 )Pch (h2 )...Pch (hn )= Pgl (g1 )Pgl (g2 )...Pgl (gm )The unigram probabilities of the words in the general language were computed from 242,918 EnglishAP news stories in the TIPSTER corpus. The stories contain 135,150,288 words from a vocabulary of428,633 distinct words.The process by which the noisy channel generates a story from a headline can be represented by aHidden Markov Model (HMM) (Baum, 1972). An HMM is a weighted finite-state automaton in whicheach state probabilistically emits a string. The simplest HMM for generating headlines consists of twostates: an H state that emits words that occur in the headline and a G state that emits all the otherwords in the story.Since we use a bigram model of headlines, each state that emits headline words must “remember”the previously emitted headline word. If we did not constrain headline words to actually occur in thestory, we would need an H state for each word in the headline vocabulary. However, because headlinewords are chosen from the story words, it is sufficient to have an H state for each story word. For anystory, the HMM consists of a start state S, an end state E, an H state for each word in the story, acorresponding G state for each H state, and a state Gstart that emits words that occur before the firstheadline word in the story. An H state can emit only the word it represents. The corresponding Gstate remembers which word was emitted by its H state and can emit any word in the story language.A headline corresponds to a path through the HMM from S to E that emits all the words in the storyin the correct order. In practice the HMM is constructed with states for only the first N words of thestory, where N is a constant (60), or N is the number of words in the first sentence.2In example (1i), given earlier, the H states will emit the words in bold (pilots, not, allowed, to,have, guns, in, cockpits) and the G states will emit all the other words. The HMM will transitionbetween the H and G states as needed to generate the words of the story. In the current example,the model will have states Start, Gstart , End, and 28 H states with 28 corresponding G states.3 Theheadline given in example (1ii) corresponds to the following sequence of states: Start, Gstart 17 times,Hpilots , Gpilots , Hnot , Gnot , Hallowed , Hto , Hhave , Hguns , Hin , Gin , Hcockpits , End. This path is not theonly one that could generate the story in (1i). Other possibilities are:(20) (i)(ii)Transportation Department decided airline pilots not to have guns.Months of the terrorist has to have cockpits.2Limiting consideration of headline words to the early part of the story is justified in Dorr et al. (2003a) where itwas shown that more than half of the headline words are chosen from the first sentence of the story. Other methods forselecting the window of story words are possible and will be explored in future research.3The subscript of a G state indicates the H state it is associated with, not the story word it emits. In the example,Gpilots emits story word will , Gnot emits story word be, and Gin emits story word the.12Although (20i) and (20ii) are possible headlines for (1i), the conditional probability of (20ii) given (1i)will be lower than the conditional probability of (20i) given (1i).The Viterbi algorithm (Viterbi, 1967) is used to select the most likely headline for a given story.We use length constraints to find the most likely headlines consisting of W words, where W rangesfrom 5 to 15. Multiple backpointers are used so that we can find the n most likely headlines at eachlength.HMM Hedge is enhanced by three additional decoding parameters to help the system choose outputsthat best mimic actual headlines: a position bias, a clump bias, and a gap bias. The incorporationof these biases changes the score produced by the decoder from a probability to a relative desirabilityscore. The three parameters were motivated by analysis of system output and their values were set bytrial and error. A logical extension to this work would be to learn the best setting of these biases, e.g.,through Expectation Maximization.The position bias favors headlines that include words near the front of the story. This reflects ourobservations of human-constructed headlines, in which headline words tend to appear near the frontof the story. The initial position bias p is a positive number less than one. The story word in thenth position is assigned a position bias of log(pn ). When an H state emits a story word, the positionbias is added to the desirability score. Thus, words near the front of the story carry less of a positionbias than words farther along. Note that this generalization often does not hold in the case of humaninterest and sports stories, which may start with a hook to get the reader’s attention, rather than atopic sentence.We also observed that human-constructed headlines tend to contain contiguous blocks of storywords. Example (1ii), given earlier, illustrates this with the string “allowed to have guns”. Thestring bias is used to favor “clumpiness”. i.e., the tendency to generate headlines composed of clumpsof contiguous story words. The log of the clump bias is added to the desirability score with eachtransition from an H state to its associated G state. With high clump biases, the system will favorheadlines consisting of fewer but larger clumps of contiguous story words.The gap bias is used to disfavor headline “gappiness”, i.e., large gaps of non-headline words in thestory between clumps of headline words. Although humans are capable of constructing fluent headlinesby selecting widely spaced words, we observed that HMM Hedge was more likely to combine unrelatedmaterial by doing this. At each transition from a G state to an H state, corresponding to the endof a sequence of non-headline words in the story, a gap bias is applied that increases with the size ofthe gap between the current headline and the last headline word to be emitted. This can also be seenas a penalty for spending too much time in one G state. With high gap biases, the system will favorheadlines with few large gaps.One characteristic difference between newspaper headline text and newspaper story text is thatheadlines tend to be in present tense while story sentences tend to be in the past tense. Past tenseverbs occur more rarely in the headline language than in the general language. HMM Hedge mimics thisaspect of Headlinese by allowing morphological variation between headline verbs and the correspondingstory verbs. Morphological variation for verbs is achieved by creating an H state for each availablevariant of a story verb. These H states still emit the story verb but they are labeled with the variant.HMM Hedge can generate a headline in which proposes is the unobserved headline word that emits theobserved story word proposed , even though proposes does not occur in the story.(21) (i)(ii)A group has proposed awarding $1 million in every general election to one randomly chosenvoter.Group proposes awarding $1 million to randomly chosen voter.Finally, we constrain each headline to contain at least one verb. That is to say, we ignore headlinesthat do not contain at least one verb, no matter how desirable the decoding is.13Although we have described an application of HMM Hedge to blocks of story words without referenceto sentence boundaries, it is also possible to use HMM as a single sentence compressor by limiting theblock to the words in a single sentence.Also, as we will see shortly, multiple alternative compressions of a sentence may be generated withHMM Hedge. The Viterbi algorithm is capable of discovering n-best compressions of a window ofstory words and can be constrained to consider only paths that include a specific number of H states,corresponding to compressions that contain a specific number of words. We use HMM Hedge to generate55 compressions for each sentence by computing the five best headlines at each length, from 5 to 15words. In Section 4 we will describe how HMM Hedge is used as a component of a multi-documentsummarization system.4Multi-Document SummarizationThe sentence compression tools we developed for single-document summarization have been incorporated into our Multi-Candidate Reduction framework for multi-document summarization. MCRproduces a textual summary from a collection of relevant documents in three steps. First, sentencesare selected from the source documents for compression. The most important information occurs nearthe front of the stories, so we select the first five sentences of each document for compression. Second,multiple compressed versions of each sentence are produced using Trimmer or HMM Hedge to create apool of candidates for inclusion in the summary. Finally, a sentence selector constructs the summaryby iteratively choosing from the pool of candidates based on a linear combination of features until thesummary reaches a desired length.At present, weights for the features are determined by manually optimizing on a set of trainingdata to maximize the Rouge-2 recall score (Lin and Hovy, 2003), using Rouge version 1.5.5. Atypical summarization task might call for the system to generate a 250-word summary from a coupleof dozen news stories. These summaries may be query-focused, in the sense that the summaries shouldbe responsive to a particular information need, or they may be generic, in that a broad overview of thedocuments is desired.Our sentence selector adopts certain aspects of Maximal Marginal Relevance (MMR) (Carbonelland Goldstein, 1998), an approach that attempts to balance relevance and anti-redundancy. In MCR’sselection module, the highest scoring sentence from the pool of eligible candidates is chosen for inclusionin the summary. Features that contribute to a candidate’s score can be divided into two types: dynamicand static. When a candidate is chosen, all other compressed variants of that sentence are eliminated.After a candidate is added to the summary, the dynamic features are re-computed, and the candidatesare re-ranked. Candidates are added to the summary until the desired length is achieved. The orderingof candidates in the summary is the same as the order in which they were selected for inclusion. Thefinal sentence of the summary is truncated if it causes the summary to exceed the length limit.4.1Static FeaturesStatic features are calculated before sentence selection begins and do not change during the process ofsummary construction:• Position. The zero-based position of the sentence in the document.• Sentence Relevance. The relevance score of the sentence to the query.• Document Relevance. The relevance score of the document to the query.• Sentence Centrality. The centrality score of the sentence to the topic cluster.14• Document Centrality. The centrality score of the document to the topic cluster.• Scores from the Compression Modules:– Trim rule application counts. For Trimmer-based MCR, the number of Trimmer rule instances applied to produce the candidate.– Negative Log Desirability. For HMM-based MCR, the relative desirability score of thecandidate.We use the Uniform Retrieval Architecture (URA), University of Maryland’s software infrastructurefor information retrieval tasks, to compute relevance and centrality scores for each compressed candidate. There are four such scores: the relevance score between a compressed sentence and the query,the relevance score between the document containing the compressed sentence and the query, the centrality score between a compressed sentence and the topic cluster, and the centrality score betweenthe document containing the compressed sentence and the topic cluster. We define the topic clusterto be the entire collection of documents relevant to this particular summarization task. Centrality isa concept that quantifies how similar a piece of text is to all other texts that discuss the same generaltopic. We assume that sentences having higher term overlap with the query and sources more “central”to the topic cluster are preferred for inclusion in the final summary.The relevance score between a compressed sentence and the query is an idf-weighted count ofoverlapping terms (number of terms shared by the two text segments). Inverse document frequency(idf), a commonly-used measure in the information retrieval literature, roughly captures term salience.The idf of a term t is defined by log(N/ct ), where N is the total number of documents in a particularcorpus and ct is the number of documents containing term t; these statistics were calculated from oneyear’s worth of LA Times articles. Weighting term overlap by inverse document frequency capturesthe intuition that matching certain terms is more important than matching others.Lucene, a freely-available off-the-shelf information retrieval system, is used to compute the threeother scores. The relevance score between the document containing the compressed sentence andthe query is computed using Lucene’s built-in similarity function. The centrality score between thecompressed sentence and the topic cluster is the mean of the similarity between the sentence and eachdocument comprising the cluster (once again, as computed by Lucene’s built-in similarity function).The document-cluster centrality score is also computed in much the same way, by taking the meanof the similarity of the particular document with every other document in the cluster. In order toobtain an accurate distribution of term frequencies to facilitate the similarity calculation, we indexedall relevant documents (i.e., the topic cluster) along with a comparable corpus (one year of the LATimes)—this additional text essentially serves as a background model for non-relevant documents.Some features are derived from the sentence compression modules used to generate candidates. ForTrimmer, the rule application count feature of a candidate is the number of rules that were applied toa source sentence to produce the candidate. The rules are not presumed to be equally effective, so therule application counts are broken down by rule type. For HMM Hedge, we use the relative desirabilityscore calculated by the decoder, expressed as a negative log.The features discussed in this section are assigned to the candidates before summary generationbegins and remain fixed throughout the process of summary sentence selection. The next sectiondiscusses how candidate features are assigned new values as summary geneneration proceeds.4.2Dynamic FeaturesDynamic features change during the process of sentence selection to reflect changes in the state of thesummary as sentences are added.4 The dynamic features are:4At present the dynamic features are properties of the candidates, calculated with respect to the current summarystate. There are no features directly relating to the amount of space left in the summary, so there is no mechanism that15• Redundancy. A measure of how similar the sentence is to the current summary.• Sentence-from-doc. The number of sentences already selected from the sentence’s document.The intuition behind our redundancy measure is that candidates containing words that occur muchmore frequently in the current state of the summary than they do in general English are redundantto the summary. We imagine that sentences in the summary are generated by the underlying worddistribution of the summary rather than the distribution of words in the general language. If a sentenceappears to have been generated by the summary rather than by the general language, we take it tobe redundant to the summary. Suppose we have a summary about earthquakes. The presence in acandidate of words like earthquake, seismic, and Richter Scale, which have a high likelihood in thesummary, will make us think that the candidate is redundant to the summary.To estimate the extent to which a candidate is more likely to have been generated by a summarythan by the general language, we consider the probabilities of the words in the candidate. We estimatethat the probability that a word w occurs in a candidate generated by the summary isP (w) = λP (w|D) + (1 − λ)P (w|C)where D is the summary, C is the general language corpus5 , λ is a parameter estimating the probabilitythat the word was generated by the summary and (1−λ) is the probability that the word was generatedby the general language. We have set λ = 0.3, as a general estimate of the portion of words in a textthat are specific to the text’s topic. We estimate the probabilities by counting the words6 in the currentsummary and the general language corpus:P (w|D) =count of w in Dsize of DP (w|C) =count of w in Csize of CWe take the probability of a sentence to be the product of the probabilities of its words, so we calculatethe probability that a sentence was generated by the summary, i.e. our redundancy metric, as:YRedundancy(S) =λP (s|D) + (1 − λ)P (s|C)s∈SFor ease of computation, we actually use log probabilities:Xlog(λP (s|D) + (1 − λ)P (s|C))s∈SRedundancy is a dynamic feature because the word distribution of the current summary changes withevery iteration of the sentence selector.4.3Examples of System OutputWe applied our MCR framework to test data from the DUC-2006 evaluation (Dang and Harman, 2006).Given a topic description and a set of 25 documents related to the topic (drawn from AP newswire,the New York Times, and the Xinhua News Agency English Service), the system’s task was to createwould affect the distribution of compressed candidates over the iterations of the sentence selector. This issue will beaddressed as future work in Section 7.5The documents in the set being summarized are used to estimate the general language model.6Actually, preprocessing for redundancy includes stopword removal and applying the Porter Stemmer (Porter, 1980).16Title: Native American Reservation System—pros and consNarrative Description: Discuss conditions on American Indian reservations or among NativeAmerican communities. Include the benefits and drawbacks of the reservation system. Includelegal privileges and problems.Figure 2: Topic D0601A from the DUC-2006 multi-document summarization task.a 250-word summary that addressed the information need expressed in the topic. One of the topicdescriptions is shown in Figure 2. The 25 documents in the document set have an average size of 1170words, so a 250-word summary represents a compression ratio of 0.86%.Figures 3, 4 and 5 show examples of MCR output using Trimmer compression, HMM Hedge compression, or no compression. For readability, we use ◦ as a sentence delimiter; this is not part of theactual system output. The sentences compressed by Trimmer mimic Headlinese by omitting determiners and auxiliary verbs. For example, the first sentence in Figure 3 is a compression of the followingsource sentence:Seeking to get a more accurate count of the country’s American Indian population, theCensus Bureau is turning to tribal leaders and residents on reservations to help overcomelong-standing feelings of wariness or anger toward the federal government.Three determiners and a form of be have been removed from the source sentence in the compressionthat appears in the summary. The removal of this material makes the sentence appear more like aheadline.In comparison with Trimmer compressions, HMM compressions are generally less readable andmore likely to be misleading. Consider the final sentence in Figure 4.(22) main purpose of reservation to pay American Indians by poverty proposalsThis is a compression of the following source sentence:(23) But the main purpose of the visit—the first to a reservation by a president since FranklinRoosevelt—was simply to pay attention to American Indians, who are so raked by grindingpoverty that Clinton’s own advisers suggested he come up with special proposals geared specifically to the Indians’ plight.Because HMM Hedge uses a bigram model of Headlines, it is unable to capture sentence-levelgrammaticality. The same limitation makes it difficult to prevent misleading or incorrect compressions.For example, the third sentence from the end of Figure 4 seems to say that a court legalized gamblingon Indian reservations:(24) Supreme Court allows legalized gambling Indian reservationsHowever, it is a compression of the following source sentence:(25) Only Monday, the California Supreme Court overturned a ballot measure that would have allowedexpansion of legalized gambling on Indian reservations.Nevertheless, we can see from the examples that sentence compression allows a summary to includemore material from other sources. This increases the topic coverage of system output.17Seeking to get more accurate count of country’s American Indian population, Census Bureau turning totribal leaders and residents on reservations to help overcome long-standing feelings. ◦ American Indianreservations would get infusion. ◦ Smith and thousands seeking help for substance abuse at AmericanIndian Community House, largest of handful of Native American cultural institutions. ◦ Clinton goingto Pine Ridge Reservation for visit with Oglala Sioux nation and to participate in conference on NativeAmerican homeownership and economic development. ◦ Said Glen Revere, nutritionist with Indian HealthServices on 2.8 million-acre Wind River Reservation, about 100 miles east of Jackson, Wyo. “Then wecame up with idea for this community garden, and it been bigger than we ever expected.” ◦ Road leadinginto Shinnecock Indian reservation is not welcoming one But main purpose of visit – first to reservationby president since Franklin Roosevelt – was simply to pay attention to American Indians, who raked bygrinding poverty Clinton’s own advisers suggested he come up with special proposals geared specifically toIndians’ plight. ◦ “This highlights what going on out there, since beginning of reservation system,” saidSidney Harring, professor at City University of New York School of Law and expert on Indian crime andcriminal law. ◦ American Indians are victims. ◦ President Clinton turned attention to arguably poorest,most forgotten ◦ U.S. citizens: American Indians. ◦ When American Indians began embracing gambling,Hualapai tribe moved quickly to open casino. ◦ members of Northern Arapaho Tribe on Wind RiverReservation started seven-acre community garden with donated land, seeds andFigure 3: MCR Summary for DUC-2006 Topic D0601A, using Trimmer for sentence compression.David Rocchio deputy legal counsel to Vermont Gov. Howard Dean who has been involved in discussionson Indian gambling through the National Governors Association said that the concern that governors haveis not with the benefit casinos bring to tribes ◦ Native Americans living on reservations that maintain 50percent or more unemployment are exempt from the national five year family limit on welfare benefits ◦Smith and thousands like her are seeking help for their substance abuse at the American Indian CommunityHouse the largest of a handful of Native American cultural institutions in the New York area ◦ Juvenilecrime is one strand in the web of social problems facing urban and reservation Indian communities thereport said ◦ Soldierwolf’s family represents the problems that plague many of the 1.3 million AmericanIndians who live on reservations of whom 49 percent are unemployed ◦ Powless said the Onondaga peoplewant to work with the community outside the reservation to improve the economy of the region perhapscreating tourism destinations that might include Indian culture or setting up a free trade zone at unusedmanufacturing sites ◦ As Indian communities across the nation struggle with short funds and a long listof problems they are watching the Navajo Nation’s legal battle with the federal government ◦ recognizeIndians not only Native Americans as Americans ◦ go on reservation system Harring Indian ◦ SupremeCourt allows legalized gambling Indian reservations ◦ American Indian reservations tribal colleges rise fasterthan ◦ main purpose of reservation to pay American Indians by poverty proposalsFigure 4: MCR Summary for DUC-2006 Topic D0601A, using HMM Hedge for sentence compression18Seeking to get a more accurate count of the country’s American Indian population, the Census Bureau isturning to tribal leaders and residents on reservations to help overcome long-standing feelings of warinessor anger toward the federal government. ◦ American Indian reservations would get an infusion of $1.2billion in federal money for education, health care and law enforcement under President Clinton’s proposed2001 budget ◦ Smith and thousands like her are seeking help for their substance abuse at the AmericanIndian Community House, the largest of a handful of Native American cultural institutions in the NewYork area. ◦ Clinton was going to the Pine Ridge Reservation for a visit with the Oglala Sioux nationand to participate in a conference on Native American homeownership and economic development. ◦ saidGlen Revere, a nutritionist with the Indian Health Services on the 2.8 million-acre Wind River Reservation,about 100 miles east of Jackson, Wyo. “Then we came up with the idea for this community garden, andit’s been bigger than we ever expected in so many ways.” ◦ The road leading into the Shinnecock Indianreservation is not a welcoming one ◦ But the main purpose of the visit – the first to a reservation by apresident since Franklin Roosevelt – was simply to pay attention to American Indians, who are so raked bygrinding poverty that Clinton’s own advisers suggested he come up with special proposals geared specificallyto the Indians’ plight. ◦ “This highlights what has been going on out there for 130 years,Figure 5: MCR Summary for DUC-2006 Topic D0601A, with no sentence compression19R1 RecallR1 PrecisionR1 FR2 RecallR2 PrecisionR2 FHMMSentence0.23552(0.230140.24082)0.21896(0.213860.22384)0.22496(0.219830.22978)0.06838(0.065460.07155)0.06287(0.060170.06576)0.06488(0.062090.06785)HMM60 Block0.21381(0.209120.21827)0.18882(0.184440.19301)0.19966(0.195050.20391)0.06133(0.058480.06414)0.05351(0.050970.05588)0.05686(0.054200.05942)TrimmerTopiary0.21014(0.204360.21594)0.20183(0.196270.20722)0.20179(0.196120.20718)0.06337(0.060300.06677)0.06230(0.058870.06617)0.06079(0.057880.06401)0.25143(0.246320.25663)0.23038(0.225670.23522)0.23848(0.233730.24328)0.06637(0.063450.06958)0.06024(0.057470.06326)0.06252(0.059760.06561)Table 2: Rouge scores and 95% confidence intervals for 624 documents from DUC-2003 test set.5System EvaluationsWe tested four single-document summarization systems on the DUC-2003 Task 1 test set:• HMM Hedge using the first sentence of each document (HMM Sentence)• HMM Hedge using the first 60 words of each document (HMM 60 block)• Trimmer• TopiaryTask 1 from DUC-2003 was to construct generic 75-byte summaries for 624 documents drawn from APNewswire and the New York Times. The average size of the documents was 3,997 bytes, so a 75-bytesummary represents a compression ratio of 1.9%.An automatic summarization evaluation tool, Rouge (Lin and Hovy, 2003), was used to evaluatethe results. The system parameters were optimized by hand to maximize the Rouge-1 recall ona comparable training corpus, 500 AP Newswire and New York Times articles from the DUC-2004single-document short summary test data.The Rouge results are shown in Table 2. Results show that HMM Hedge 60 scored significantlylower than most other systems and that Topiary scored higher than all other systems for all R1measures. In addition, HMM Hedge Sentence scored significantly higher than Trimmer for the R1measures.We also evaluated Trimmer and HMM Hedge as components in our Multi-Candidate Reductionframework, along with a baseline that uses the same sentence selector but does not use sentencecompression. All three systems considered the first five sentences of each document and used thesentence selection algorithm presented in Section 4. The feature weights were manually optimized tomaximize Rouge-2 recall on a comparable training corpus, 1,593 Financial Times and Los AngelesTimes articles grouped into 50 topics from the DUC-2005 query-focused multi-document summarization20R1 RecallR2 RecallTrimmerHMM Hedge0.29391(0.285600.30247)0.06718(0.063320.07111)0.27311(0.265540.28008)0.06251(0.058730.06620)NoCompression0.27576(0.267720.28430)0.06126(0.057670.06519)Table 3: Rouge scores and 95% confidence intervals for 50 DUC-2006 test topics, comparing threeMCR variants.MCR ScoreHigherNot DifferentRangeLowerRouge-20.08051230.0678-0.089911Rouge-SU40.13601240.1238-0.147510BE-HM0.04130270.0318-0.05088Table 4: Official DUC-2006 Automatic Metrics for our MCR submission (System 32).test data. The systems were used to generate query-focused, 250-word summaries using the DUC-2006test data, described in Section 4.3.The systems were evaluated using Rouge, configured to omit stopwords from the calculation.7Results are shown in Table 3. MCR using Trimmer compressions scored significantly higher thanMCR using HMM Hedge compressions and the baseline for Rouge-1, but there was not a significantdifference among the three systems for Rouge-2.Finally, the University of Maryland and BBN submitted a version of MCR to the official DUC2006 evaluation. This version used Trimmer as the source of sentence compressions. Results showthat use of sentence compression hurt the system on human evaluation of grammaticality. This is notsurprising, since Trimmer aims to produce compressions that are grammatical in Headlinese, ratherthan standard English. Our MCR run scored significantly lower than 23 systems on NIST’s humanevaluation of grammaticality. However, the system did not score significantly lower than any othersystem on NIST’s human evaluation of content responsiveness. A second NIST evaluation of contentresponsiveness asked evaluators to take readability into consideration. In this evaluation, MCR scoredsignificantly lower than only two systems. The evaluators recognized that Trimmer compressions arenot grammatical in standard English; yet, the content coverage was not significantly different from thebest automatic systems and only two systems were found to be significantly more readable.NIST computed three “official” automatic evaluation metrics for DUC-2006: Rouge-2, RougeSU4 and BE-HM. Table 4 shows the official scores of the submitted MCR system for these threemetrics, along with numbers of systems that scored significantly higher, significantly lower, or were notsignificantly different from our MCR run. Also shown is the range of scores for the systems that werenot significantly different from MCR. These results show that the performance of our MCR run wascomparable to most other systems submitted to DUC-2006.7This is a change in the Rouge configuration from the official DUC-2006 evaluation. We note that the removal ofnon-essential stopwords (typical of Headlinese) is an important component of Trimmer-based sentence compression. Forinternal system comparisons, we configure Rouge in a way that will allow us to detect system differences relevant to ourresearch focus. For reporting of official Rouge results on submitted systems we use the community’s accepted Rougeconfigurations.21The evaluation in Table 3 suggests that Trimmer sentence compression is preferable to HMMHedge sentence compression for generation of English summaries of collections of document in English.However, HMM Hedge may prove to have value with noisier data, as we discuss in the next section.Nevertheless, sentence compression appears to be a valuable component of our framework for multidocument summarization, thus validating the ideas behind Multi-Candidate Reduction.6Applications to Different Types of TextsWe have applied the MCR framework to summarizing different types of texts. In this section we brieflytouch on genre-specific issues that are the subject of ongoing work. Trimmer, Topiary, and HMM Hedgewere designed for summarization of written news. In this genre, the lead sentence is almost alwaysthe first non-trivial sentence of the document. More sophisticated methods for finding lead sentencesdid not outperform the baseline of simply selecting the first sentence for AP wire “hard” news stories.However, some types of articles, such as sports stories, opinion pieces, and movie reviews often donot have informative lead sentences and will require additional work in finding the best sentence forcompression.MCR has also been applied to summarizing transcripts of broadcast news—another input formwhere lead sentences are often not informative. The conventions of broadcast news introduce categoriesof story-initial light content sentences, such as “I’m Dan Rather” or “We have an update on the storywe’ve been following”. These present challenges for the filtering stage of our MCR framework.Such texts are additionally complicated by a range of problems not encountered in written news:noise introduced by automatic speech recognizers or other faulty transcription, issues associated withsentence boundary detection and story boundary detection. If word error rate is high, parser failurescan prevent Trimmer from producing useful output. In this context, HMM Hedge becomes moreattractive, since our language models are more resilient to noisy input.We have performed an initial evaluation of Trimmer, Topiary, and a baseline consisting of thefirst 75 characters of a document, on the task of creating 75-character headlines for broadcast newstranscriptions (Zajic, 2007). The corpus for this task consisted of 560 broadcast news stories fromABC, CNN, NBC, Public Radio International, and Voice of America. We used Rouge-1 recall toevaluate the summaries and found that both systems scored higher than the baseline and that Topiaryscored higher than Trimmer. However there were no significant differences among the systems.Another application of our framework is the summarization of email threads—collections of emailsthat share a common topic or were written as responses to each other. This task can essentially betreated as a multi-document summarization problem, albeit email thread structure introduces someconstraints with respect to the ordering of summary sentences. Noisy data is inherent in this problemand pre-processing to remove quoted text, attachments, and headers is crucial. We have found thatmetadata, such as the name of the sender of each included extract help make email summaries easierto read.We performed an initial evaluation of HMM Hedge and Trimmer as the source of sentence compressions for an email thread summarization system based on the MCR framework (Zajic, 2007). Thecorpus for this task consisted of 10 manually constructed email threads from the Enron Corpus (Klimtand Yang, 2004). We used Rouge-1 and Rouge-2 recall with jackknifing to compare the automaticsystems and the human summarizers. We did not observe a significant difference between the two systems, but we found that the task of summarizing email threads was extremely difficult for the humans(one summarizer scored significantly worse than the automatic systems). This application of MCR toemail thread summarization is an initial effort. The difficulty of the task for the humans suggests thatthe community needs to develop a clearer understanding of what makes a good email thread summaryand to explore practical uses for them.22Finally, Trimmer and HMM Hedge have been applied to Hindi-English cross-language summarization. In this case, Trimmer was applied to the output of machine translation. We adapted HMMHedge to cross-lingual summarization by using the mechanism developed for morphological variationto represent translation probabilities from Hindi story words to English headline words. For moredetails, see Dorr et al. (2003a).7Future WorkFuture work on text summarization under the Multi-Candidate Reduction framework will focus on thethree main components of the architecture: sentence filtering, sentence compression, and candidateselection.For single document summarization, the simple technique of selecting the first non-trivial sentenceof a document for compression remains the best approach. However, for human interest stories or sportsarticles, this approach is less effective. In broadcast news transcripts, the first sentence often does notcontain important information. Currently, filtering for multi-document summarization also relies onthe assumption that important information tends to appear near the front of documents—the first fivesentences of each document are retained to generate compressed candidates. An interesting area offuture work is to explore other approaches to filtering, such as using query relevance and documentcentrality, to move beyond the baseline of selecting the first n sentences. For HMM Hedge, thesemethods can be used to determine the optimal blocks of text on which to apply the decoder.Currently, Trimmer produces multiple compressions by applying rules in a fixed order; the state ofthe compressed sentence after each rule application becomes a candidate. A richer pool of candidatescan be produced by modifying Trimmer rules to operate in order-independent combinations, ratherthan a fixed sequence. We believe that the sentence selector can produce better summaries if it haslarger pools of candidates to choose from. Naturally, different sentence compressions are not the onlytechniques for enriching the candidate pool—other possibilities include merging sentences and resolvinganaphora. Topiary will also be enhanced by using multiple combinations of compressions and topicterms in the context of headline generation.We also plan to enrich the candidate selector by taking into account more features of the currentsummary state. Possibilities include sentence selector iteration count and remaining summary space, aswell as feature weights that change during the progress of summary generation. These extensions willallow us to study the distribution of compressed and uncompressed sentences across sentence selectoriterations. System output can potentially be improved by finer-grained control of this distribution.These features might also help avoid the current problem in which the final sentence is truncated dueto length restrictions (e.g., by selecting a final sentence of more appropriate length).Proper setting of parameters is another important area for future work. Systematic optimizationof parameter values in HMM Hedge and the sentence selector could lead to significant improvementsin output quality. A logical extension to this work would be to learn the best parameter settings, e.g.,through Expectation Maximization.At present, MCR focuses exclusively on summary content selection and does not take sentenceordering into consideration when constructing the summary. Naturally, high-quality summaries shouldread fluently in addition to having relevant content. Recent work in this area that can be appliedto MCR includes includes Conroy et al. (2006), Barzilay et al. (2002), Okazaki et al. (2004), Lapata (2003), and Dorr and Gaasterland (this special issue 2007). Within the MCR architecture, fluencyconsiderations can be balanced with other important factors such as relevance and anti-redundancythrough appropriate feature weighting.238ConclusionThis work presents Multi-Candidate Reduction, a general architecture for multi-document summarization. The framework integrates successful single-document compression techniques that we havepreviously developed. MCR is motivated by the insight that multiple candidate compressions of sourcesentences should be made available to subsequent processing modules, which may have access to moreinformation for summary construction. This is implemented in a dynamic feature-based sentence selector that iteratively builds a summary from compressed variants. Evaluations show that sentencecompression plays an important role in multi-document summarization and that our MCR frameworkis both flexible and extensible.AcknowledgmentsThis work has been supported, in part, under the GALE program of the Defense Advanced ResearchProjects Agency, Contract No. HR0011-06-2-0001, the TIDES program of the Defense AdvancedResearch Projects Agency, BBNT Contract No. 9500006806, and the University of Maryland JointInstitute for Knowledge Discovery. Any opinions, findings, conclusions or recommendations expressedin this paper are those of the authors and do not necessarily reflect the views of DARPA. The firstauthor would like to thank Naomi for proofreading, support, and encouragement. The second authorwould like to thank Steve, Carissa, and Ryan for their energy enablement. The third author would liketo thank Esther and Kiri for their kind support.ReferencesL. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum likelihood approach to continuous speechrecognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.M. Banko, V. Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation.In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL2000), pages 318–325, Hong Kong.R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal of Artificial Intelligence Research, 17:35–55.L. Baum. 1972. An inequality and associated maximization technique in statistical estimation ofprobabilistic functions of a Markov process. Inequalities, 3:1–8.S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor coreferenceresolution for text summarization. In Proceedings of the HLT-NAACL 2003 Text SummarizationWorkshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta.D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. MachineLearning, 34(1/3):211–231.S. Blair-Goldensohn, D. Evans, V.B. Schiffman, A. Schlaikjer, A.at DUC 2004. In Proceedings ofHLT/NAACL 2004, pages 23–30,Hatzivassiloglou, K. McKeown, A. Nenkova, R. Passonneau,Siddharthan, and S. Siegelman. 2004. Columbia Universitythe 2004 Document Understanding Conference (DUC 2004) atBoston, Massachusetts.24P. Brown, J. Cocke, S. Pietra, V. Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin. 1990. Astatistical approach to machine translation. Computational Linguistics, 16(2):79–85.J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reorderingdocuments and producing summaries. In Proceedings of the 21st Annual International ACM SIGIRConference on Research and Development in Information Retrieval (SIGIR 1998), pages 335–336,Melbourne, Australia.Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the First Meetingof the North American Chapter of the Association for Computational Linguistics (NAACL 2000),pages 132–139, Seattle, Washington.J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains,training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for ComputationalLinguistics (COLING/ACL 2006), pages 377–384, Sydney, Australia.J. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document summarization. In Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP2005, Vancouver, Canada.J. Conroy, J. Schlesinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: CLASSY 2006. InProceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006,New York, New York.D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of theThird Conference on Applied Natural Language Processing, Trento, Italy.Hoa Dang and Donna Harman. 2006. Proceedings of the 2006 Document Understanding Conference(DUC 2006) at HLT/NAACL 2006.B. Dorr and T. Gaasterland. this special issue, 2007. Exploiting aspectual features and connectingwords for summarization-inspired temporal-relation extraction. Information Processing and Management.B. Dorr, D. Zajic, and R. Schwartz. 2003a. Cross-language headline generation for Hindi. ACMTransactions on Asian Language Information Processing (TALIP), 2(3):270–289.B. Dorr, D. Zajic, and R. Schwartz. 2003b. Hedge Trimmer: A parse-and-trim approach to headlinegeneration. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and DocumentUnderstanding Conference (DUC 2003), pages 1–8, Edmonton, Alberta.T. Dunning. 1994. Statistical identification of language. Technical Report MCCS 94-273, New MexicoState University.T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13thInternational Workshop on Database and Expert Systems Applications (DEXA 2002), pages 215–222, Aix-en-Provence, France.J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization bysentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summarization, pages 40–48.D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC),Philadelphia.25H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the FirstMeeting of the North American Chapter of the Association for Computational Linguistics (NAACL2000), pages 178–185, Seattle, Washington.B. Klimt and Y. Yang. 2004. Introducing the Enron Corpus. In Proceedings of the First Conferenceon Email and Anti-Spam (CEAS), Mountain View, California.K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. InProceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI-2000), Austin,Texas.K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approachto sentence compression. Artificial Intelligence, 139(1):91–107.M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedingsof the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages545–552, Barcelona, Spain.David Dolan Lewis. 1999. An evaluation of phrasal and clustered representations on a text categorization task. In Proceedings of the 15th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR 1992), pages 37–50, Copenhagen, Denmark.C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Human Language Technology Conference and the North AmericanChapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003),pages 71–78, Edmonton, Alberta.I. Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo.E. Mays, F. Damerau, and R. Mercer. 1990. Context-based spelling correction. In Proceedings of IBMNatural Language ITL, pages 517–522, Paris, France.S. Miller, L. Ramshaw, H. Fox, and R. Weischedel. 2000. A novel use of statistical parsing to extractinformation from text. In Proceedings of the First Meeting of the North American Chapter of theAssociation for Computational Linguistics (NAACL 2000), pages 226–233, Seattle, Washington.S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learningtechniques for email. In Proceedings of the ACL/EACL 2001 Workshop on Computational NaturalLanguage Learning (ConLL), pages 290–297, Toulouse, France.N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving chronological sentence ordering by precedence relation. In Proceedings of the 20th International Conference on Computational Linguistics(COLING 2004), pages 750–756, Geneva, Switzerland.M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. Çelebi, S. Dimitrov, E. Drabek, A. Hakim,W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel, M. Topper, A. Winkel, and Z. Zhang.2004. MEAD—a platform for multidocument multilingual text summarization. In Proceedings ofthe 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon,Portugal.26R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood modelfor topic classification of broadcast news. In Proceedings of the Fifth European Speech Communication Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes,Greece.S. Sista, R. Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discoveryfrom broadcast news stories. In Proceedings of the 2002 Human Language Technology Conference(HLT), pages 99–103, San Diego, California.J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression.In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL2005), pages 290–297, Ann Arbor, Michigan.L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at DUC2006: Task-focusedsummarization with sentence simplification and lexical expansion. In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York.A. Viterbi. 1967. Error bounds for convolution codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260–269.R. Wang, N. Stokes, W. Doran, E. Newman, J. Carthy, and J. Dunnion. 2005. Comparing Topiarystyle approaches to headline generation. In Lecture Notes in Computer Science: Advances inInformation Retrieval: 27th European Conference on IR Research (ECIR 2005), volume 3408,Santiago de Compostela, Spain. Springer Berlin / Heidelberg.D. Zajic, B. Dorr, and R. Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings ofthe 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119,Boston, Massachusetts.D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at MSE2005. In Proceedings of theMSE2005 Track of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures forMT and/or Summarization, Ann Arbor, Michigan.D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J. Lin. 2005b. A sentence-trimming approach tomulti-document summarization. In Proceedings of the 2005 Document Understanding Conference(DUC-2005) at NLT/EMNLP 2005, pages 151–158, Vancouver, Canada.D. Zajic. 2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Summarization Tasks. Ph.D. thesis, University of Maryland, College Park.L. Zhou and E. Hovy. 2003. Headline summarization at ISI. In Proceedings of the HLT-NAACL2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages174–178, Edmonton, Alberta.27</biblio>
</article>
