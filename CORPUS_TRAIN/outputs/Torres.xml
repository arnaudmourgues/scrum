<article>
	<preamble>Torres.pdf</preamble>
	<author></author>
	<titre>Summary Evaluationwith and without ReferencesJuan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales</titre>
	<abstract></abstract>
	<biblio>while other measures compare peers with all or some of theinput material:MEASUREM (SUMi,k , Ii0 )3) Update-summarization task that consists of creating asummary out of a cluster of documents and a topic. Twosub-tasks are considered here: A) an initial summary hasto be produced based on an initial set of documents andtopic; B) an update summary has to be produced froma different (but related) cluster assuming documentsused in A) are known. The English TAC’08 UpdateSummarization dataset is used, which consists of 48topics with 20 documents each – 36,911 words.4) Opinion summarization where systems have to analyzea set of blog articles and summarize the opinionsabout a target in the articles. The TAC’08 OpinionSummarization in English4 data set (taken from theBlogs06 Text Collection) is used: 25 clusters and targets(i.e., target entity and questions) were used – 1,167,735words.5) Generic single-document summarization in Spanishusing the Medicina Clı́nica5 corpus, which is composedof 50 medical articles in Spanish, each one with itscorresponding author abstract – 124,929 words.6) Generic single document summarization in French usingthe “Canadien French Sociological Articles” corpusfrom the journal Perspectives interdisciplinaires sur letravail et la santé (PISTES)6 . It contains 50 sociologicalarticles in French, each one with its correspondingauthor abstract – 381,039 words.7) Generic multi-document-summarization in French usingdata from the RPM27 corpus [18], 20 different themesconsisting of 10 articles and 4 abstracts by referencethematic – 185,223 words.(4)where Ii0 is some subset of input Ii . The values producedby the measures for each summary SUMi,k are averagedfor each system k = 0, . . . , s − 1 and these averages areused to produce a ranking. Rankings are then comparedusing Spearman Rank correlation [17] which is used tomeasure the degree of association between two variableswhose values are used to rank objects. We have chosento use this correlation to compare directly results to thosepresented in [12]. Computation of correlations is done usingthe Statistics-RankCorrelation-0.12 package1 , which computesthe rank correlation between two vectors. We also verifiedthe good conformity of the results with the correlation testof Kendall τ calculated with the statistical software R. Thetwo nonparametric tests of Spearman and Kendall do notreally stand out as the treatment of ex-æquo. The goodcorrespondence between the two tests shows that they do notintroduce bias in our analysis. Subsequently will mention onlythe ρ of Sperman more widely used in this field.A. ToolsWe carry out experimentation using a new summarizationevaluation framework: F RESA –FRamework for EvaluatingSummaries Automatically–, which includes document-basedsummary evaluation measures based on probabilitiesdistribution2 . As in the ROUGE package, F RESA supportsdifferent n-grams and skip n-grams probability distributions.The F RESA environment can be used in the evaluation ofsummaries in English, French, Spanish and Catalan, and itintegrates filtering and lemmatization in the treatment ofsummaries and documents. It is developed in Perl and willbe made publicly available. We also use the ROUGE package[10] to compute various ROUGE statistics in new datasets.For experimentation in the TAC and the DUC datasets we usedirectly the peer summaries produced by systems participatingin the evaluations. For experimentation in Spanish and French(single and multi-document summarization) we have createdsummaries at a similar ratio to those of reference using thefollowing systems:– ENERTEX [19], a summarizer based on a theory oftextual energy;– CORTEX [20], a single-document sentence extractionsystem for Spanish and French that combines variousstatistical measures of relevance (angle between sentenceand topic, various Hamming weights for sentences, etc.)and applies an optimal decision algorithm for sentenceselection;– SUMMTERM [21], a terminology-based summarizer thatis used for summarization of medical articles anduses specialized terminology for scoring and rankingsentences;– REG [22], summarization system based on an greedyalgorithm;B. Summarization Tasks and Data SetsWe have conducted our experimentation with the followingsummarization tasks and data sets:1) Generic multi-document-summarization in English(production of a short summary of a cluster of relateddocuments) using data from DUC’043 , task 2: 50clusters, 10 documents each – 294,636 words.2) Focused-based summarization in English (production ofa short focused multi-document summary focused on thequestion “who is X?”, where X is a person’s name) usingdata from the DUC’04 task 5: 50 clusters, 10 documentseach plus a target person name – 284,440 words.4 http://www.nist.gov/tac/data/index.html1 http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/5 http://www.elsevier.es/revistas/ctl2 F RESAis available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/Ressources.html3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.htmlservlet? f=7032&revistaid=26 http://www.pistes.uqam.ca/7 http://www-labs.sinequa.com/rpm215Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales– J S summarizer, a summarization system that scoresand ranks sentences according to their Jensen-Shannondivergence to the source document;– a lead-based summarization system that selects the leadsentences of the document;– a random-based summarization system that selectssentences at random;– Open Text Summarizer [23], a multi-lingual summarizerbased on the frequency and– commercial systems: Word, SSSummarizer8 , Pertinence9and Copernic10 .presented here we used uni-grams, 2-grams, and the skip2-grams with maximum skip distance of 4 (ROUGE-1,ROUGE-2 and ROUGE-SU4). ROUGE is used to comparea peer summary to a set of model summaries in ourframework (as indicated in equation 3).– Jensen-Shannon divergence formula given in Equation 2is implemented in our F RESA package with the followingspecification (Equation 6) for the probability distributionof words w.CTPw = wN(SCwifw∈SNSQw =(6)TCw+δotherwiseN +δ∗BC. Evaluation MeasuresThe following measures derived from human assessment ofthe content of the summaries are used in our experiments:– C OVERAGE is understood as the degree to which onepeer summary conveys the same information as a modelsummary [2]. C OVERAGE was used in DUC evaluations.This measure is used as indicated in equation 3 usinghuman references or models.– R ESPONSIVENESS ranks summaries in a 5-point scaleindicating how well the summary satisfied a giveninformation need [2]. It is used in focused-basedsummarization tasks. This measure is used as indicatedin equation 4 since a human judges the summarywith respect to a given input “user need” (e.g., aquestion). R ESPONSIVENESS was used in DUC and TACevaluations.– P YRAMIDS [11] is a content assessment measure whichcompares content units in a peer summary to weightedcontent units in a set of model summaries. Thismeasure is used as indicated in equation 3 using humanreferences or models. P YRAMIDS is the adopted metricfor content-based evaluation in the TAC evaluations.For DUC and TAC datasets the values of these measures areavailable and we used them directly. We used the followingautomatic evaluation measures in our experiments:– ROUGE [14], which is a recall metric that takes intoaccount n-grams as units of content for comparing peerand model summaries. The ROUGE formula specified in[10] is as follows:PmROUGE-n(R, M ) =P∈ M n−gram∈P countmatch (n − gram)PPcount(n-gram)m ∈MWhere P is the probability distribution of words w intext T and Q is the probability distribution of words win summary S; N is the number of words in text andTsummary N = NT +NS , B = 1.5|V |, Cwis the numberSof words in the text and Cw is the number of words inthe summary. For smoothing the summary’s probabilitieswe have used δ = 0.005. We have also implementedother smoothing approaches (e.g. Good-Turing [24], thatuses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2package11 ) in F RESA, but we do not use them inthe experiments reported here. Following the ROUGEapproach, in addition to word uni-grams we use 2-gramsand skip n-grams computing divergences such as J S(using uni-grams) J S 2 (using 2-grams), J S 4 (using theskip n-grams of ROUGE-SU4), and J S M which is anaverage of the J S i . J Ss measures are used to compare apeer summary to its source document(s) in our framework(as indicated in equation 4). In the case of summarizationof multiple documents, these are concatenated (in thegiven input order) to form a single input from whichprobabilities are computed.IV. E XPERIMENTS AND R ESULTSWe first replicated the experiments presented in [12] toverify that our implementation of J S produced correlationresults compatible with that work. We used the TAC’08Update Summarization data set and computed J S andROUGE measures for each peer summary. We producedtwo system rankings (one for each measure), which werecompared to rankings produced using the manual P YRAMIDSand R ESPONSIVENESS scores. Spearman correlations werecomputed among the different rankings. The results arepresented in Table I. These results confirm a high correlationamong P YRAMIDS, R ESPONSIVENESS and J S. We alsoverified high correlation between J S and ROUGE-2 (0.83Spearman correlation, not shown in the table) in this task anddataset.Then, we experimented with data from DUC’04, TAC’08Opinion Summarization pilot task as well as single and(5)where R is the summary to be evaluated, M is the set ofmodel (human) summaries, countmatch is the number ofcommon n-grams in m and P , and count is the numberof n-grams in the model summaries. For the experiments8 http://www.kryltech.com/summarizer.htm9 http://www.pertinence.net11 http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/10 http://www.copernic.com/en/products/summarizerPolibits (42) 201016Summary Evaluation with and without ReferencesTABLE IS PEARMANMesureROUGE-2JSCORRELATION OF CONTENT- BASED MEASURES INU PDATE S UMMARIZATION TASKP YRAMIDS0.960.85p-valuep < 0.005p < 0.005R ESPONSIVENESS0.920.74evaluation metrics that do not rely on human models but thatcompare summary content to input content directly [12]. Wehave some positive and some negative results regarding thedirect use of the full document in content-based evaluation.We have verified that in both generic muti-documentsummarizationandintopic-basedmulti-documentsummarization in English correlation among measuresthat use human models (P YRAMIDS, R ESPONSIVENESSand ROUGE) and a measure that does not use models(J S divergence) is strong. We have found that correlationamong the same measures is weak for summarization ofbiographical information and summarization of opinions inblogs. We believe that in these cases content-based measuresshould be considered, in addition to the input document, thesummarization task (i.e. text-based representation, description)to better assess the content of the peers [25], the task being adeterminant factor in the selection of content for the summary.Our multi-lingual experiments in generic single-documentsummarization confirm a strong correlation among theJ S divergence and ROUGE measures. It is worth notingthat ROUGE is in general the chosen framework forpresenting content-based evaluation results in non-Englishsummarization.For the experiments in Spanish, we are conscious that weonly have one model summary to compare with the peers.Nevertheless, these models are the corresponding abstractswritten by the authors. As the experiments in [26] show, theprofessionals of a specialized domain (as, for example, themedical domain) adopt similar strategies to summarize theirtexts and they tend to choose roughly the same content chunksfor their summaries. Previous studies have shown that authorabstracts are able to reformulate content with fidelity [27] andthese abstracts are ideal candidates for comparison purposes.Because of this, the summary of the author of a medical articlecan be taken as reference for summaries evaluation. It is worthnoting that there is still debate on the number of models to beused in summarization evaluation [28]. In the French corpusPISTES, we suspect the situation is similar to the Spanishcase.TAC’08p-valuep < 0.005p < 0.005multi-document summarization in Spanish and French. In spiteof the fact that the experiments for French and Spanish corporause less data points (i.e., less summarizers per task) thanfor English, results are still quite significant. For DUC’04,we computed the J S measure for each peer summary intasks 2 and 5 and we used J S, ROUGE, C OVERAGE andR ESPONSIVENESS scores to produce systems’ rankings. Thevarious Spearman’s rank correlation values for DUC’04 arepresented in Tables II (for task 2) and III (for task 5).For task 2, we have verified a strong correlation betweenJ S and C OVERAGE. For task 5, the correlation betweenJ S and C OVERAGE is weak, and that between J S andR ESPONSIVENESS is weak and negative.Although the Opinion Summarization (OS) task is a newtype of summarization task and its evaluation is a complicatedissue, we have decided to compare J S rankings with thoseobtained using P YRAMIDS and R ESPONSIVENESS in TAC’08.Spearman’s correlation values are listed in Table IV. As it canbe seen, there is weak and negative correlation of J S withboth P YRAMIDS and R ESPONSIVENESS. Correlation betweenP YRAMIDS and R ESPONSIVENESS rankings is high for thistask (0.71 Spearman’s correlation value).For experimentation in mono-document summarizationin Spanish and French, we have run 11 multi-lingualsummarization systems; for experimentation in French, wehave run 12 systems. In both cases, we have producedsummaries at a compression rate close to the compression rateof the authors’ provided abstracts. We have then computed J Sand ROUGE measures for each summary and we have averagedthe measure’s values for each system. These averages wereused to produce rankings per each measure. We computedSpearman’s correlations for all pairs of rankings.Results are presented in Tables V, VI and VII. All resultsshow medium to strong correlation between the J S measuresand ROUGE measures. However the J S measure based onuni-grams has lower correlation than J Ss which use n-gramsof higher order. Note that table VII presents results forgeneric multi-document summarization in French, in thiscase correlation scores are lower than correlation scores forsingle-document summarization in French, a result which maybe expected given the diversity of input in multi-documentsummarization.VI. C ONCLUSIONS AND F UTURE W ORKThis paper has presented a series of experiments incontent-based measures that do not rely on the use of modelsummaries for comparison purposes. We have carried outextensive experimentation with different summarization tasksdrawing a clearer picture of tasks where the measures couldbe applied. This paper makes the following contributions:– We have shown that if we are only interested in rankingsummarization systems according to the content of theirautomatic summaries, there are tasks were models couldbe subtituted by the full document in the computation ofthe J S measure obtaining reliable rankings. However,we have also found that the substitution of modelsby full-documents is not always advisable. We haveV. D ISCUSSIONThe departing point for our inquiry into text summarizationevaluation has been recent work on the use of content-based17Polibits (42) 2010Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-MoralesTABLE IIS PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2MesureROUGE-2JSC OVERAGE0.790.68p-valuep < 0.0050p < 0.0025TABLE IIIS PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5MesureROUGE-2JSC OVERAGE0.780.40p-valuep < 0.001p < 0.050R ESPONSIVENESS0.44-0.18p-valuep < 0.05p < 0.25TABLE IVS PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OSMesureJSP YRAMIDS-0.13p-valuep < 0.25R ESPONSIVENESS-0.14TASKp-valuep < 0.25TABLE VS PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH )MesureJSJ S2J S4J SMROUGE -10.560.880.880.82p-valuep < 0.100p < 0.001p < 0.001p < 0.005ROUGE -20.460.800.800.71ROUGE -SU40.450.810.810.71p-valuep < 0.200p < 0.005p < 0.005p < 0.010a representation of the task/topic in the calculation ofmeasures. To carry out these comparisons, however, we aredependent on the existence of references.F RESA will also be used in the new question-answer taskcampaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/qa.asp) for the evaluation of long answers. This task aimsto answer a question by extraction and agglomeration ofsentences in Wikipedia. This kind of task correspondsto those for which we have found a high correlationamong the measures J S and evaluation methods withhuman intervention. Moreover, the J S calculation will beamong the summaries produced and a representative set ofrelevant passages from Wikipedia. F RESA will be used tocompare three types of systems, although different tasks: themulti-document summarizer guided by a query, the searchsystems targeted information (focused IR) and the questionanswering systems.found weak correlation among different rankings incomplex summarization tasks such as the summarizationof biographical information and the summarization ofopinions.– We have also carried out large-scale experiments inSpanish and French which show positive medium tostrong correlation among system’s ranks produced byROUGE and divergence measures that do not use themodel summaries.– We have also presented a new framework, F RESA, forthe computation of measures based on J S divergence.Following the ROUGE approach, F RESA package useword uni-grams, 2-grams and skip n-grams computingdivergences. This framework will be available to thecommunity for research purposes.Although we have made a number of contributions, this paperleaves many open questions than need to be addressed. Inorder to verify correlation between ROUGE and J S, in theshort term we intend to extend our investigation to otherlanguages such as Portuguese and Chinesse for which wehave access to data and summarization technology. We alsoplan to apply F RESA to the rest of the DUC and TACsummarization tasks, by using several smoothing techniques.As a novel idea, we contemplate the possibility of adaptingthe evaluation framework for the phrase compression task[29], which, to our knowledge, does not have an efficientevaluation measure. The main idea is to calculate J S froman automatically-compressed sentence taking the completesentence by reference. In the long term, we plan to incorporatePolibits (42) 2010p-valuep < 0.100p < 0.002p < 0.002p < 0.020ACKNOWLEDGMENTWe are grateful to the Programa Ramón y Cajal fromMinisterio de Ciencia e Innovación, Spain. This work ispartially supported by: a postdoctoral grant from the NationalProgram for Mobility of Research Human Resources (NationalPlan of Scientific Research, Development and Innovation2008-2011, Ministerio de Ciencia e Innovación, Spain); theresearch project CONACyT, number 82050, and the researchproject PAPIIT-DGAPA (Universidad Nacional Autónoma deMéxico), number IN403108.18Summary Evaluation with and without ReferencesTABLE VIS PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE PISTES C ORPUS (F RENCH )MesureJSJ S2J S4J SMROUGE -10.700.930.830.88p-valuep < 0.050p < 0.002p < 0.020p < 0.010ROUGE -20.730.860.760.83p-valuep < 0.05p < 0.01p < 0.05p < 0.02ROUGE -SU40.730.860.760.83p-valuep < 0.500p < 0.005p < 0.050p < 0.010TABLE VIIS PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE RPM2 C ORPUS (F RENCH )MeasureJSJ S2J S4J SMROUGE -10.8300.8000.7500.850p-valuep < 0.002p < 0.005p < 0.010p < 0.002ROUGE -20.6600.5900.5200.640R EFERENCESp-valuep < 0.05p < 0.05p < 0.10p < 0.05ROUGE -SU40.7410.6800.6200.740p-valuep < 0.01p < 0.02p < 0.05p < 0.01[18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,“A French Human Reference Corpus for multi-documentssummarization and sentence compression,” in LREC’10, vol. 2,Malta, 2010, p. In press.[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energyof Associative Memories: performants applications of Enertex algorithmin text summarization and topic segmentation,” in MICAI’07, 2007, pp.861–871.[20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,“Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,St Malo, France, 2002, pp. 723–734.[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,“Automatic summarization using terminological and semanticresources,” in LREC’10, vol. 2, Malta, 2010, p. In press.[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme gloutonappliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,p. In press.[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modernsystems of automatic text summarization,” Automatic Documentationand Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.[24] C. D. Manning and H. Schütze, Foundations of Statistical NaturalLanguage Processing.Cambridge, Massachusetts: The MIT Press,1999.[25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,vol. 43, no. 6, pp. 1449–1481, 2007.[26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specializeddiscourse: The case of medical articles in spanish,” Terminology, vol. 13,no. 2, pp. 249–286, 2007.[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACLStudent Research Workshop.Toulouse, France: Association forComputational Linguistics, 9-11 July 2001 2001, pp. 49–54.[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,Singapore, August 2009, pp. 23–30.[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:Sentence compression,” in Proceedings of the National Conference onArtificial Intelligence. Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999, 2000, pp. 703–710.[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andB. Sundheim, “Summac: a text summarization evaluation,” NaturalLanguage Engineering, vol. 8, no. 1, pp. 43–68, 2002.[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,no. 6, pp. 1506–1520, 2007.[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,USA: NIST, November 17-19 2008.[4] K. Spärck Jones and J. Galliers, Evaluating Natural LanguageProcessing Systems, An Analysis and Review, ser. Lecture Notes inComputer Science. Springer, 1996, vol. 1083.[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison ofrankings produced by summarization evaluation measures,” in NAACLWorkshop on Automatic Summarization, 2000, pp. 69–78.[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluationof Summaries in a Cross-lingual Environment using Content-basedMetrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,D. Liu, and E. Drábek, “Evaluation challenges in large-scale documentsummarization,” in ACL’03, 2003, pp. 375–382.[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a methodfor automatic evaluation of machine translation,” in ACL’02, 2002, pp.311–318.[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in EvaluationInitiatives in Natural Language Processing. Budapest, Hungary: EACL,14 April 2003.[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation ofSummaries,” in Text Summarization Branches Out: ACL-04 Workshop,M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection inSummarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.145–152.[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selectionin Summarization without Human Models,” in Empirical Methods inNatural Language Processing, Singapore, August 2009, pp. 306–314.[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEETransactions on Information Theory, vol. 37, no. 145-151, 1991.[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries UsingN-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,USA: Association for Computational Linguistics, 2003, pp. 71–78.[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoreticapproach to automatic evaluation of summaries,” in HLT-NAACL,Morristown, USA, 2006, pp. 463–470.[16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. ofMath. Stat., vol. 22, no. 1, pp. 79–86, 1951.[17] S. Siegel and N. Castellan, Nonparametric Statistics for the BehavioralSciences. McGraw-Hill, 1998.19Polibits (42) 2010</biblio>
</article>
